{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jow5c8X5hU-Y",
        "outputId": "2ee66a16-393b-4887-af78-f376ee2b1344"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "^C\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: gradio in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (4.31.5)\n",
            "Requirement already satisfied: transformers in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (4.41.1)\n",
            "Requirement already satisfied: requests in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (2.31.0)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.29.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: pydub in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.23.2)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (10.3.0)\n",
            "Requirement already satisfied: ffmpy in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.3.2)\n",
            "Requirement already satisfied: gradio-client==0.16.4 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.16.4)\n",
            "Requirement already satisfied: urllib3~=2.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (2.2.1)\n",
            "Requirement already satisfied: orjson~=3.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (3.10.3)\n",
            "Requirement already satisfied: pydantic>=2.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (2.7.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (5.3.0)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (3.8.4)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.4.5)\n",
            "Requirement already satisfied: fastapi in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.111.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: packaging in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: numpy~=1.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: fsspec in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio-client==0.16.4->gradio) (2024.3.1)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio-client==0.16.4->gradio) (11.0.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from transformers) (2024.5.10)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: filelock in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from requests) (2024.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from requests) (3.7)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (4.22.0)\n",
            "Requirement already satisfied: toolz in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: sniffio in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: anyio in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (4.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.51.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.18.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: rich>=10.11.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from fastapi->gradio) (2.1.1)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from fastapi->gradio) (0.0.3)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from fastapi->gradio) (5.9.0)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from fastapi->gradio) (0.37.2)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (0.21.0)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (0.19.0)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (0.6.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: gradio in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (4.31.5)\n",
            "Requirement already satisfied: packaging in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (6.4.0)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (3.8.4)\n",
            "Requirement already satisfied: pydub in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.27.0)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: urllib3~=2.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (2.2.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: pydantic>=2.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (2.7.1)\n",
            "Requirement already satisfied: ffmpy in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.3.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (5.3.0)\n",
            "Requirement already satisfied: numpy~=1.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.4.5)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (10.3.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.23.2)\n",
            "Requirement already satisfied: gradio-client==0.16.4 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.16.4)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.12.3)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.29.0)\n",
            "Requirement already satisfied: fastapi in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.111.0)\n",
            "Requirement already satisfied: orjson~=3.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (3.10.3)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: fsspec in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio-client==0.16.4->gradio) (2024.3.1)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from gradio-client==0.16.4->gradio) (11.0.3)\n",
            "Requirement already satisfied: toolz in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (4.22.0)\n",
            "Requirement already satisfied: idna in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (3.7)\n",
            "Requirement already satisfied: certifi in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2024.2.2)\n",
            "Requirement already satisfied: sniffio in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.3.1)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: anyio in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (4.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (3.14.0)\n",
            "Requirement already satisfied: requests in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: cycler>=0.10 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.1.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.51.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.2.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.18.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.7.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from fastapi->gradio) (0.37.2)\n",
            "Requirement already satisfied: ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from fastapi->gradio) (5.9.0)\n",
            "Requirement already satisfied: fastapi-cli>=0.0.2 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from fastapi->gradio) (0.0.3)\n",
            "Requirement already satisfied: email_validator>=2.0.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from fastapi->gradio) (2.1.1)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from email_validator>=2.0.0->fastapi->gradio) (2.6.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.35.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from anyio->httpx>=0.24.1->gradio) (1.2.1)\n",
            "Requirement already satisfied: httptools>=0.5.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (0.6.1)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (0.21.0)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (0.19.0)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "# Download packages\n",
        "\n",
        "!pip install -U -q python-dotenv\n",
        "!pip install -U -q langchain\n",
        "!pip install -U -q langchain_community\n",
        "!pip install -U -q langchain_experimental\n",
        "!pip install gradio transformers requests\n",
        "!pip install -U -q huggingface_hub\n",
        "!pip install gradio\n",
        "!pip install -U -q chromadb\n",
        "!pip install requests beautifulsoup4\n",
        "!pip install -U -q langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "yPwftnogE9gP",
        "outputId": "49b065f2-3b25-430c-acac-995ace4de147"
      },
      "outputs": [],
      "source": [
        "# Connect to google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "\n",
        "import os\n",
        "os.chdir('/content/gdrive/MyDrive/Colab Notebooks/Computational Linguistics/Final Project/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9815081",
        "outputId": "43b2bbc9-5e3e-497b-812b-c74a161195f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2fe4840a-e737-461c-ac7b-a1f33ab56199\n",
            "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\n"
          ]
        }
      ],
      "source": [
        "# Read API Key\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# 失敗的話會列印出：'沒讀到 HUGGINGFACEHUB_API key'\n",
        "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN', '沒讀到 HUGGINGFACEHUB_API_TOKEN')\n",
        "GRAPHHOPPER_API_TOKEN = os.getenv('GRAPHHOPPER_API_TOKEN', '沒讀到 GRAPHHOPPER_API_TOKEN')\n",
        "\n",
        "print(GRAPHHOPPER_API_TOKEN)\n",
        "print(HUGGINGFACEHUB_API_TOKEN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 用一個網頁試試看語言模型回應"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# List of URLs to extract text from\n",
        "urls = [\n",
        "    'https://spectralcodex.com/daxi-jianxin-theater/',\n",
        "    'https://spectralcodex.com/zhongli-dadong-theater/',\n",
        "    'https://spectralcodex.com/zhongli-guobin-commercial-building/',\n",
        "    # Add more URLs as needed\n",
        "]\n",
        "\n",
        "# Open the output file in write mode\n",
        "with open('all_webpages_text.txt', 'w', encoding='utf-8') as file:\n",
        "    for url in urls:\n",
        "        # Send a GET request to the URL\n",
        "        response = requests.get(url)\n",
        "        \n",
        "        # Check if the request was successful\n",
        "        if response.status_code == 200:\n",
        "            # Parse the HTML content of the page\n",
        "            soup = BeautifulSoup(response.content, 'html.parser')\n",
        "            \n",
        "            # Extract all text from the page\n",
        "            text = soup.get_text(separator=' ', strip=True)\n",
        "            \n",
        "            # Write the URL and the extracted text to the file\n",
        "            file.write(f\"URL: {url}\\n\")\n",
        "            file.write(text + \"\\n\\n\")  # Add a newline to separate each page's text\n",
        "            \n",
        "            print(f\"Text extracted and saved for {url}\")\n",
        "        else:\n",
        "            print(f\"Failed to retrieve the web page {url}. Status code: {response.status_code}\")\n",
        "\n",
        "print(\"All texts have been extracted and saved to all_webpages_text.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#讀取長文本\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader('./blog_posts_to_p10.txt')\n",
        "doc = loader.load()\n",
        "print (f\"You have {len(doc)} document\")\n",
        "print (f\"You have {len(doc[0].page_content)} characters in that document\")\n",
        "\n",
        "#切分長文本\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=100)\n",
        "splitted_docs = text_splitter.split_documents(doc)\n",
        "\n",
        "\n",
        "num_total_characters = sum([len(x.page_content) for x in splitted_docs])\n",
        "\n",
        "print(f\"我們有 {len(splitted_docs)} splitted documents\")\n",
        "print(f\"平均每個 splitted document 有 {num_total_characters / len(splitted_docs):,.0f} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the embeddings engine ready\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "my_embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-l6-v2\",\n",
        "    api_key=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],\n",
        ")\n",
        "# The vectorstore we use: Chroma\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "my_vectorstore = Chroma(embedding_function = my_embeddings,\n",
        "                        persist_directory = './Chroma_DATABASE')\n",
        "# Embed the splitted documents and add it to my vectorstore\n",
        "\n",
        "my_vectorstore.add_documents(splitted_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 定義 retriever\n",
        "# 要求他只返回最相似(最可能和query相關) 的 2 個 document\n",
        "retriever = my_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "# 測試用 retriever 搜尋看看\n",
        "#retriever.invoke('theater')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "## 定義 prompt\n",
        "template = \"\"\"根據給定 context 回答問題:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "## 定義模型\n",
        "llm = HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "                          max_length=128, temperature=0.5,)\n",
        "\n",
        "\n",
        "## 多加了一個整理 splitted doc 的小函數\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "\n",
        "## 定義 chain\n",
        "chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 這一格用來爬整個網站"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching page 1: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/1/\n",
            "Fetching page 2: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/2/\n",
            "Fetching page 3: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/3/\n",
            "Fetching page 4: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/4/\n",
            "Fetching page 5: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/5/\n",
            "Fetching page 6: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/6/\n",
            "Fetching page 7: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/7/\n",
            "Fetching page 8: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/8/\n",
            "Fetching page 9: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/9/\n",
            "Fetching page 10: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/10/\n",
            "Fetching page 11: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/11/\n",
            "Fetching page 12: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/12/\n",
            "Fetching page 13: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/13/\n",
            "Fetching page 14: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/14/\n",
            "Fetching page 15: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/15/\n",
            "Fetching page 16: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/16/\n",
            "Fetching page 17: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/17/\n",
            "Fetching page 18: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/18/\n",
            "Fetching page 19: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/19/\n",
            "Fetching page 20: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/20/\n",
            "Fetching page 21: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/21/\n",
            "Fetching page 22: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/22/\n",
            "Fetching page 23: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/23/\n",
            "Fetching page 24: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/24/\n",
            "Fetching page 25: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/25/\n",
            "Fetching page 26: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/26/\n",
            "Fetching page 27: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/27/\n",
            "Fetching page 28: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/28/\n",
            "Fetching page 29: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/29/\n",
            "Fetching page 30: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/30/\n",
            "Fetching page 31: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/31/\n",
            "Fetching page 32: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/32/\n",
            "Fetching page 33: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/33/\n",
            "Fetching page 34: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/34/\n",
            "Fetching page 35: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/35/\n",
            "Fetching page 36: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/36/\n",
            "Fetching page 37: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/37/\n",
            "Fetching page 38: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/38/\n",
            "Fetching page 39: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/39/\n",
            "Fetching page 40: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/40/\n",
            "Fetching page 41: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/41/\n",
            "Fetching page 42: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/42/\n",
            "Fetching page 43: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/43/\n",
            "Fetching page 44: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/44/\n",
            "Fetching page 45: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/45/\n",
            "Fetching page 46: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/46/\n",
            "Fetching page 47: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/47/\n",
            "Fetching page 48: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/48/\n",
            "Fetching page 49: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/49/\n",
            "Fetching page 50: https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/page/50/\n",
            "Processing https://aniseblog.tw/265819\n",
            "Content extracted and saved for https://aniseblog.tw/265819\n",
            "Processing https://aniseblog.tw/265807\n",
            "Content extracted and saved for https://aniseblog.tw/265807\n",
            "Processing https://aniseblog.tw/264948\n",
            "Content extracted and saved for https://aniseblog.tw/264948\n",
            "Processing https://aniseblog.tw/264640\n",
            "Content extracted and saved for https://aniseblog.tw/264640\n",
            "Processing https://aniseblog.tw/264357\n",
            "Content extracted and saved for https://aniseblog.tw/264357\n",
            "Processing https://aniseblog.tw/263984\n",
            "Content extracted and saved for https://aniseblog.tw/263984\n",
            "Processing https://aniseblog.tw/263654\n",
            "Content extracted and saved for https://aniseblog.tw/263654\n",
            "Processing https://aniseblog.tw/263194\n",
            "Content extracted and saved for https://aniseblog.tw/263194\n",
            "Processing https://aniseblog.tw/262588\n",
            "Content extracted and saved for https://aniseblog.tw/262588\n",
            "Processing https://aniseblog.tw/262147\n",
            "Content extracted and saved for https://aniseblog.tw/262147\n",
            "Processing https://aniseblog.tw/261968\n",
            "Content extracted and saved for https://aniseblog.tw/261968\n",
            "Processing https://aniseblog.tw/261937\n",
            "Content extracted and saved for https://aniseblog.tw/261937\n",
            "Processing https://aniseblog.tw/261831\n",
            "Content extracted and saved for https://aniseblog.tw/261831\n",
            "Processing https://aniseblog.tw/261438\n",
            "Content extracted and saved for https://aniseblog.tw/261438\n",
            "Processing https://aniseblog.tw/261240\n",
            "Content extracted and saved for https://aniseblog.tw/261240\n",
            "Processing https://aniseblog.tw/260681\n",
            "Content extracted and saved for https://aniseblog.tw/260681\n",
            "Processing https://aniseblog.tw/260541\n",
            "Content extracted and saved for https://aniseblog.tw/260541\n",
            "Processing https://aniseblog.tw/260351\n",
            "Content extracted and saved for https://aniseblog.tw/260351\n",
            "Processing https://aniseblog.tw/259823\n",
            "Content extracted and saved for https://aniseblog.tw/259823\n",
            "Processing https://aniseblog.tw/259611\n",
            "Content extracted and saved for https://aniseblog.tw/259611\n",
            "Processing https://aniseblog.tw/259407\n",
            "Content extracted and saved for https://aniseblog.tw/259407\n",
            "Processing https://aniseblog.tw/258684\n",
            "Content extracted and saved for https://aniseblog.tw/258684\n",
            "Processing https://aniseblog.tw/258387\n",
            "Content extracted and saved for https://aniseblog.tw/258387\n",
            "Processing https://aniseblog.tw/258225\n",
            "Content extracted and saved for https://aniseblog.tw/258225\n",
            "Processing https://aniseblog.tw/258034\n",
            "Content extracted and saved for https://aniseblog.tw/258034\n",
            "Processing https://aniseblog.tw/257430\n",
            "Content extracted and saved for https://aniseblog.tw/257430\n",
            "Processing https://aniseblog.tw/257180\n",
            "Content extracted and saved for https://aniseblog.tw/257180\n",
            "Processing https://aniseblog.tw/257221\n",
            "Content extracted and saved for https://aniseblog.tw/257221\n",
            "Processing https://aniseblog.tw/256610\n",
            "Content extracted and saved for https://aniseblog.tw/256610\n",
            "Processing https://aniseblog.tw/256360\n",
            "Content extracted and saved for https://aniseblog.tw/256360\n",
            "Processing https://aniseblog.tw/256473\n",
            "Content extracted and saved for https://aniseblog.tw/256473\n",
            "Processing https://aniseblog.tw/256183\n",
            "Content extracted and saved for https://aniseblog.tw/256183\n",
            "Processing https://aniseblog.tw/256021\n",
            "Content extracted and saved for https://aniseblog.tw/256021\n",
            "Processing https://aniseblog.tw/255405\n",
            "Content extracted and saved for https://aniseblog.tw/255405\n",
            "Processing https://aniseblog.tw/255358\n",
            "Content extracted and saved for https://aniseblog.tw/255358\n",
            "Processing https://aniseblog.tw/255016\n",
            "Content extracted and saved for https://aniseblog.tw/255016\n",
            "Processing https://aniseblog.tw/254801\n",
            "Content extracted and saved for https://aniseblog.tw/254801\n",
            "Processing https://aniseblog.tw/254662\n",
            "Content extracted and saved for https://aniseblog.tw/254662\n",
            "Processing https://aniseblog.tw/253852\n",
            "Content extracted and saved for https://aniseblog.tw/253852\n",
            "Processing https://aniseblog.tw/253314\n",
            "Content extracted and saved for https://aniseblog.tw/253314\n",
            "Processing https://aniseblog.tw/253820\n",
            "Content extracted and saved for https://aniseblog.tw/253820\n",
            "Processing https://aniseblog.tw/253624\n",
            "Content extracted and saved for https://aniseblog.tw/253624\n",
            "Processing https://aniseblog.tw/253100\n",
            "Content extracted and saved for https://aniseblog.tw/253100\n",
            "Processing https://aniseblog.tw/252824\n",
            "Content extracted and saved for https://aniseblog.tw/252824\n",
            "Processing https://aniseblog.tw/252505\n",
            "Content extracted and saved for https://aniseblog.tw/252505\n",
            "Processing https://aniseblog.tw/252161\n",
            "Content extracted and saved for https://aniseblog.tw/252161\n",
            "Processing https://aniseblog.tw/252071\n",
            "Content extracted and saved for https://aniseblog.tw/252071\n",
            "Processing https://aniseblog.tw/251999\n",
            "Content extracted and saved for https://aniseblog.tw/251999\n",
            "Processing https://aniseblog.tw/251971\n",
            "Content extracted and saved for https://aniseblog.tw/251971\n",
            "Processing https://aniseblog.tw/251715\n",
            "Content extracted and saved for https://aniseblog.tw/251715\n",
            "Processing https://aniseblog.tw/251473\n",
            "Content extracted and saved for https://aniseblog.tw/251473\n",
            "Processing https://aniseblog.tw/250773\n",
            "Content extracted and saved for https://aniseblog.tw/250773\n",
            "Processing https://aniseblog.tw/251681\n",
            "Content extracted and saved for https://aniseblog.tw/251681\n",
            "Processing https://aniseblog.tw/251298\n",
            "Content extracted and saved for https://aniseblog.tw/251298\n",
            "Processing https://aniseblog.tw/250707\n",
            "Content extracted and saved for https://aniseblog.tw/250707\n",
            "Processing https://aniseblog.tw/250301\n",
            "Content extracted and saved for https://aniseblog.tw/250301\n",
            "Processing https://aniseblog.tw/250185\n",
            "Content extracted and saved for https://aniseblog.tw/250185\n",
            "Processing https://aniseblog.tw/250001\n",
            "Content extracted and saved for https://aniseblog.tw/250001\n",
            "Processing https://aniseblog.tw/248688\n",
            "Content extracted and saved for https://aniseblog.tw/248688\n",
            "Processing https://aniseblog.tw/249065\n",
            "Content extracted and saved for https://aniseblog.tw/249065\n",
            "Processing https://aniseblog.tw/249086\n",
            "Content extracted and saved for https://aniseblog.tw/249086\n",
            "Processing https://aniseblog.tw/249731\n",
            "Content extracted and saved for https://aniseblog.tw/249731\n",
            "Processing https://aniseblog.tw/249665\n",
            "Content extracted and saved for https://aniseblog.tw/249665\n",
            "Processing https://aniseblog.tw/249103\n",
            "Content extracted and saved for https://aniseblog.tw/249103\n",
            "Processing https://aniseblog.tw/248319\n",
            "Content extracted and saved for https://aniseblog.tw/248319\n",
            "Processing https://aniseblog.tw/249124\n",
            "Content extracted and saved for https://aniseblog.tw/249124\n",
            "Processing https://aniseblog.tw/248673\n",
            "Content extracted and saved for https://aniseblog.tw/248673\n",
            "Processing https://aniseblog.tw/248657\n",
            "Content extracted and saved for https://aniseblog.tw/248657\n",
            "Processing https://aniseblog.tw/248579\n",
            "Content extracted and saved for https://aniseblog.tw/248579\n",
            "Processing https://aniseblog.tw/248400\n",
            "Content extracted and saved for https://aniseblog.tw/248400\n",
            "Processing https://aniseblog.tw/248382\n",
            "Content extracted and saved for https://aniseblog.tw/248382\n",
            "Processing https://aniseblog.tw/248348\n",
            "Content extracted and saved for https://aniseblog.tw/248348\n",
            "Processing https://aniseblog.tw/248320\n",
            "Content extracted and saved for https://aniseblog.tw/248320\n",
            "Processing https://aniseblog.tw/247605\n",
            "Content extracted and saved for https://aniseblog.tw/247605\n",
            "Processing https://aniseblog.tw/247312\n",
            "Content extracted and saved for https://aniseblog.tw/247312\n",
            "Processing https://aniseblog.tw/246878\n",
            "Content extracted and saved for https://aniseblog.tw/246878\n",
            "Processing https://aniseblog.tw/246829\n",
            "Content extracted and saved for https://aniseblog.tw/246829\n",
            "Processing https://aniseblog.tw/246727\n",
            "Content extracted and saved for https://aniseblog.tw/246727\n",
            "Processing https://aniseblog.tw/246426\n",
            "Content extracted and saved for https://aniseblog.tw/246426\n",
            "Processing https://aniseblog.tw/245926\n",
            "Content extracted and saved for https://aniseblog.tw/245926\n",
            "Processing https://aniseblog.tw/244532\n",
            "Content extracted and saved for https://aniseblog.tw/244532\n",
            "Processing https://aniseblog.tw/244419\n",
            "Content extracted and saved for https://aniseblog.tw/244419\n",
            "Processing https://aniseblog.tw/244055\n",
            "Content extracted and saved for https://aniseblog.tw/244055\n",
            "Processing https://aniseblog.tw/243733\n",
            "Content extracted and saved for https://aniseblog.tw/243733\n",
            "Processing https://aniseblog.tw/243678\n",
            "Content extracted and saved for https://aniseblog.tw/243678\n",
            "Processing https://aniseblog.tw/243486\n",
            "Content extracted and saved for https://aniseblog.tw/243486\n",
            "Processing https://aniseblog.tw/243481\n",
            "Content extracted and saved for https://aniseblog.tw/243481\n",
            "Processing https://aniseblog.tw/243268\n",
            "Content extracted and saved for https://aniseblog.tw/243268\n",
            "Processing https://aniseblog.tw/242887\n",
            "Content extracted and saved for https://aniseblog.tw/242887\n",
            "Processing https://aniseblog.tw/242485\n",
            "Content extracted and saved for https://aniseblog.tw/242485\n",
            "Processing https://aniseblog.tw/242456\n",
            "Content extracted and saved for https://aniseblog.tw/242456\n",
            "Processing https://aniseblog.tw/242436\n",
            "Content extracted and saved for https://aniseblog.tw/242436\n",
            "Processing https://aniseblog.tw/242339\n",
            "Content extracted and saved for https://aniseblog.tw/242339\n",
            "Processing https://aniseblog.tw/242109\n",
            "Content extracted and saved for https://aniseblog.tw/242109\n",
            "Processing https://aniseblog.tw/241935\n",
            "Content extracted and saved for https://aniseblog.tw/241935\n",
            "Processing https://aniseblog.tw/241466\n",
            "Content extracted and saved for https://aniseblog.tw/241466\n",
            "Processing https://aniseblog.tw/240479\n",
            "Content extracted and saved for https://aniseblog.tw/240479\n",
            "Processing https://aniseblog.tw/240106\n",
            "Content extracted and saved for https://aniseblog.tw/240106\n",
            "Processing https://aniseblog.tw/240078\n",
            "Content extracted and saved for https://aniseblog.tw/240078\n",
            "Processing https://aniseblog.tw/239751\n",
            "Content extracted and saved for https://aniseblog.tw/239751\n",
            "Processing https://aniseblog.tw/239645\n",
            "Content extracted and saved for https://aniseblog.tw/239645\n",
            "Processing https://aniseblog.tw/239405\n",
            "Content extracted and saved for https://aniseblog.tw/239405\n",
            "Processing https://aniseblog.tw/238878\n",
            "Content extracted and saved for https://aniseblog.tw/238878\n",
            "Processing https://aniseblog.tw/239075\n",
            "Content extracted and saved for https://aniseblog.tw/239075\n",
            "Processing https://aniseblog.tw/236229\n",
            "Content extracted and saved for https://aniseblog.tw/236229\n",
            "Processing https://aniseblog.tw/234852\n",
            "Content extracted and saved for https://aniseblog.tw/234852\n",
            "Processing https://aniseblog.tw/234634\n",
            "Content extracted and saved for https://aniseblog.tw/234634\n",
            "Processing https://aniseblog.tw/234513\n",
            "Content extracted and saved for https://aniseblog.tw/234513\n",
            "Processing https://aniseblog.tw/234159\n",
            "Content extracted and saved for https://aniseblog.tw/234159\n",
            "Processing https://aniseblog.tw/234118\n",
            "Content extracted and saved for https://aniseblog.tw/234118\n",
            "Processing https://aniseblog.tw/234070\n",
            "Content extracted and saved for https://aniseblog.tw/234070\n",
            "Processing https://aniseblog.tw/234046\n",
            "Content extracted and saved for https://aniseblog.tw/234046\n",
            "Processing https://aniseblog.tw/234025\n",
            "Content extracted and saved for https://aniseblog.tw/234025\n",
            "Processing https://aniseblog.tw/233952\n",
            "Content extracted and saved for https://aniseblog.tw/233952\n",
            "Processing https://aniseblog.tw/233861\n",
            "Content extracted and saved for https://aniseblog.tw/233861\n",
            "Processing https://aniseblog.tw/233823\n",
            "Content extracted and saved for https://aniseblog.tw/233823\n",
            "Processing https://aniseblog.tw/233794\n",
            "Content extracted and saved for https://aniseblog.tw/233794\n",
            "Processing https://aniseblog.tw/233764\n",
            "Content extracted and saved for https://aniseblog.tw/233764\n",
            "Processing https://aniseblog.tw/233712\n",
            "Content extracted and saved for https://aniseblog.tw/233712\n",
            "Processing https://aniseblog.tw/233674\n",
            "Content extracted and saved for https://aniseblog.tw/233674\n",
            "Processing https://aniseblog.tw/233616\n",
            "Content extracted and saved for https://aniseblog.tw/233616\n",
            "Processing https://aniseblog.tw/233557\n",
            "Content extracted and saved for https://aniseblog.tw/233557\n",
            "Processing https://aniseblog.tw/232906\n",
            "Content extracted and saved for https://aniseblog.tw/232906\n",
            "Processing https://aniseblog.tw/233518\n",
            "Content extracted and saved for https://aniseblog.tw/233518\n",
            "Processing https://aniseblog.tw/232902\n",
            "Content extracted and saved for https://aniseblog.tw/232902\n",
            "Processing https://aniseblog.tw/232969\n",
            "Content extracted and saved for https://aniseblog.tw/232969\n",
            "Processing https://aniseblog.tw/232822\n",
            "Content extracted and saved for https://aniseblog.tw/232822\n",
            "Processing https://aniseblog.tw/232867\n",
            "Content extracted and saved for https://aniseblog.tw/232867\n",
            "Processing https://aniseblog.tw/232217\n",
            "Content extracted and saved for https://aniseblog.tw/232217\n",
            "Processing https://aniseblog.tw/231780\n",
            "Content extracted and saved for https://aniseblog.tw/231780\n",
            "Processing https://aniseblog.tw/231557\n",
            "Content extracted and saved for https://aniseblog.tw/231557\n",
            "Processing https://aniseblog.tw/231247\n",
            "Content extracted and saved for https://aniseblog.tw/231247\n",
            "Processing https://aniseblog.tw/231220\n",
            "Content extracted and saved for https://aniseblog.tw/231220\n",
            "Processing https://aniseblog.tw/231168\n",
            "Content extracted and saved for https://aniseblog.tw/231168\n",
            "Processing https://aniseblog.tw/231093\n",
            "Content extracted and saved for https://aniseblog.tw/231093\n",
            "Processing https://aniseblog.tw/231007\n",
            "Content extracted and saved for https://aniseblog.tw/231007\n",
            "Processing https://aniseblog.tw/230987\n",
            "Content extracted and saved for https://aniseblog.tw/230987\n",
            "Processing https://aniseblog.tw/230545\n",
            "Content extracted and saved for https://aniseblog.tw/230545\n",
            "Processing https://aniseblog.tw/230349\n",
            "Content extracted and saved for https://aniseblog.tw/230349\n",
            "Processing https://aniseblog.tw/229983\n",
            "Content extracted and saved for https://aniseblog.tw/229983\n",
            "Processing https://aniseblog.tw/229385\n",
            "Content extracted and saved for https://aniseblog.tw/229385\n",
            "Processing https://aniseblog.tw/228871\n",
            "Content extracted and saved for https://aniseblog.tw/228871\n",
            "Processing https://aniseblog.tw/227418\n",
            "Content extracted and saved for https://aniseblog.tw/227418\n",
            "Processing https://aniseblog.tw/226658\n",
            "Content extracted and saved for https://aniseblog.tw/226658\n",
            "Processing https://aniseblog.tw/226609\n",
            "Content extracted and saved for https://aniseblog.tw/226609\n",
            "Processing https://aniseblog.tw/225886\n",
            "Content extracted and saved for https://aniseblog.tw/225886\n",
            "Processing https://aniseblog.tw/225618\n",
            "Content extracted and saved for https://aniseblog.tw/225618\n",
            "Processing https://aniseblog.tw/224179\n",
            "Content extracted and saved for https://aniseblog.tw/224179\n",
            "Processing https://aniseblog.tw/223782\n",
            "Content extracted and saved for https://aniseblog.tw/223782\n",
            "Processing https://aniseblog.tw/223256\n",
            "Content extracted and saved for https://aniseblog.tw/223256\n",
            "Processing https://aniseblog.tw/222621\n",
            "Content extracted and saved for https://aniseblog.tw/222621\n",
            "Processing https://aniseblog.tw/222886\n",
            "Content extracted and saved for https://aniseblog.tw/222886\n",
            "Processing https://aniseblog.tw/221670\n",
            "Content extracted and saved for https://aniseblog.tw/221670\n",
            "Processing https://aniseblog.tw/221524\n",
            "Content extracted and saved for https://aniseblog.tw/221524\n",
            "Processing https://aniseblog.tw/221456\n",
            "Content extracted and saved for https://aniseblog.tw/221456\n",
            "Processing https://aniseblog.tw/220980\n",
            "Content extracted and saved for https://aniseblog.tw/220980\n",
            "Processing https://aniseblog.tw/220720\n",
            "Content extracted and saved for https://aniseblog.tw/220720\n",
            "Processing https://aniseblog.tw/220647\n",
            "Content extracted and saved for https://aniseblog.tw/220647\n",
            "Processing https://aniseblog.tw/220599\n",
            "Content extracted and saved for https://aniseblog.tw/220599\n",
            "Processing https://aniseblog.tw/220496\n",
            "Content extracted and saved for https://aniseblog.tw/220496\n",
            "Processing https://aniseblog.tw/220460\n",
            "Content extracted and saved for https://aniseblog.tw/220460\n",
            "Processing https://aniseblog.tw/220382\n",
            "Content extracted and saved for https://aniseblog.tw/220382\n",
            "Processing https://aniseblog.tw/220207\n",
            "Content extracted and saved for https://aniseblog.tw/220207\n",
            "Processing https://aniseblog.tw/220096\n",
            "Content extracted and saved for https://aniseblog.tw/220096\n",
            "Processing https://aniseblog.tw/219799\n",
            "Content extracted and saved for https://aniseblog.tw/219799\n",
            "Processing https://aniseblog.tw/219685\n",
            "Content extracted and saved for https://aniseblog.tw/219685\n",
            "Processing https://aniseblog.tw/219497\n",
            "Content extracted and saved for https://aniseblog.tw/219497\n",
            "Processing https://aniseblog.tw/219417\n",
            "Content extracted and saved for https://aniseblog.tw/219417\n",
            "Processing https://aniseblog.tw/219256\n",
            "Content extracted and saved for https://aniseblog.tw/219256\n",
            "Processing https://aniseblog.tw/219043\n",
            "Content extracted and saved for https://aniseblog.tw/219043\n",
            "Processing https://aniseblog.tw/218927\n",
            "Content extracted and saved for https://aniseblog.tw/218927\n",
            "Processing https://aniseblog.tw/218606\n",
            "Content extracted and saved for https://aniseblog.tw/218606\n",
            "Processing https://aniseblog.tw/218636\n",
            "Content extracted and saved for https://aniseblog.tw/218636\n",
            "Processing https://aniseblog.tw/218197\n",
            "Content extracted and saved for https://aniseblog.tw/218197\n",
            "Processing https://aniseblog.tw/217892\n",
            "Content extracted and saved for https://aniseblog.tw/217892\n",
            "Processing https://aniseblog.tw/217820\n",
            "Content extracted and saved for https://aniseblog.tw/217820\n",
            "Processing https://aniseblog.tw/217556\n",
            "Content extracted and saved for https://aniseblog.tw/217556\n",
            "Processing https://aniseblog.tw/217372\n",
            "Content extracted and saved for https://aniseblog.tw/217372\n",
            "Processing https://aniseblog.tw/216666\n",
            "Content extracted and saved for https://aniseblog.tw/216666\n",
            "Processing https://aniseblog.tw/216208\n",
            "Content extracted and saved for https://aniseblog.tw/216208\n",
            "Processing https://aniseblog.tw/216244\n",
            "Content extracted and saved for https://aniseblog.tw/216244\n",
            "Processing https://aniseblog.tw/216171\n",
            "Content extracted and saved for https://aniseblog.tw/216171\n",
            "Processing https://aniseblog.tw/215917\n",
            "Content extracted and saved for https://aniseblog.tw/215917\n",
            "Processing https://aniseblog.tw/215783\n",
            "Content extracted and saved for https://aniseblog.tw/215783\n",
            "Processing https://aniseblog.tw/215652\n",
            "Content extracted and saved for https://aniseblog.tw/215652\n",
            "Processing https://aniseblog.tw/215585\n",
            "Content extracted and saved for https://aniseblog.tw/215585\n",
            "Processing https://aniseblog.tw/215267\n",
            "Content extracted and saved for https://aniseblog.tw/215267\n",
            "Processing https://aniseblog.tw/214916\n",
            "Content extracted and saved for https://aniseblog.tw/214916\n",
            "Processing https://aniseblog.tw/214892\n",
            "Content extracted and saved for https://aniseblog.tw/214892\n",
            "Processing https://aniseblog.tw/214109\n",
            "Content extracted and saved for https://aniseblog.tw/214109\n",
            "Processing https://aniseblog.tw/213424\n",
            "Content extracted and saved for https://aniseblog.tw/213424\n",
            "Processing https://aniseblog.tw/212352\n",
            "Content extracted and saved for https://aniseblog.tw/212352\n",
            "Processing https://aniseblog.tw/213047\n",
            "Content extracted and saved for https://aniseblog.tw/213047\n",
            "Processing https://aniseblog.tw/212730\n",
            "Content extracted and saved for https://aniseblog.tw/212730\n",
            "Processing https://aniseblog.tw/212664\n",
            "Content extracted and saved for https://aniseblog.tw/212664\n",
            "Processing https://aniseblog.tw/212470\n",
            "Content extracted and saved for https://aniseblog.tw/212470\n",
            "Processing https://aniseblog.tw/212033\n",
            "Content extracted and saved for https://aniseblog.tw/212033\n",
            "Processing https://aniseblog.tw/211789\n",
            "Content extracted and saved for https://aniseblog.tw/211789\n",
            "Processing https://aniseblog.tw/211646\n",
            "Content extracted and saved for https://aniseblog.tw/211646\n",
            "Processing https://aniseblog.tw/211523\n",
            "Content extracted and saved for https://aniseblog.tw/211523\n",
            "Processing https://aniseblog.tw/210535\n",
            "Content extracted and saved for https://aniseblog.tw/210535\n",
            "Processing https://aniseblog.tw/209889\n",
            "Content extracted and saved for https://aniseblog.tw/209889\n",
            "Processing https://aniseblog.tw/210126\n",
            "Content extracted and saved for https://aniseblog.tw/210126\n",
            "Processing https://aniseblog.tw/209586\n",
            "Content extracted and saved for https://aniseblog.tw/209586\n",
            "Processing https://aniseblog.tw/209365\n",
            "Content extracted and saved for https://aniseblog.tw/209365\n",
            "Processing https://aniseblog.tw/209488\n",
            "Content extracted and saved for https://aniseblog.tw/209488\n",
            "Processing https://aniseblog.tw/209140\n",
            "Content extracted and saved for https://aniseblog.tw/209140\n",
            "Processing https://aniseblog.tw/209269\n",
            "Content extracted and saved for https://aniseblog.tw/209269\n",
            "Processing https://aniseblog.tw/209185\n",
            "Content extracted and saved for https://aniseblog.tw/209185\n",
            "Processing https://aniseblog.tw/208744\n",
            "Content extracted and saved for https://aniseblog.tw/208744\n",
            "Processing https://aniseblog.tw/207358\n",
            "Content extracted and saved for https://aniseblog.tw/207358\n",
            "Processing https://aniseblog.tw/206048\n",
            "Content extracted and saved for https://aniseblog.tw/206048\n",
            "Processing https://aniseblog.tw/205739\n",
            "Content extracted and saved for https://aniseblog.tw/205739\n",
            "Processing https://aniseblog.tw/205630\n",
            "Content extracted and saved for https://aniseblog.tw/205630\n",
            "Processing https://aniseblog.tw/204099\n",
            "Content extracted and saved for https://aniseblog.tw/204099\n",
            "Processing https://aniseblog.tw/203064\n",
            "Content extracted and saved for https://aniseblog.tw/203064\n",
            "Processing https://aniseblog.tw/202708\n",
            "Content extracted and saved for https://aniseblog.tw/202708\n",
            "Processing https://aniseblog.tw/202609\n",
            "Content extracted and saved for https://aniseblog.tw/202609\n",
            "Processing https://aniseblog.tw/202484\n",
            "Content extracted and saved for https://aniseblog.tw/202484\n",
            "Processing https://aniseblog.tw/202450\n",
            "Content extracted and saved for https://aniseblog.tw/202450\n",
            "Processing https://aniseblog.tw/202319\n",
            "Content extracted and saved for https://aniseblog.tw/202319\n",
            "Processing https://aniseblog.tw/201727\n",
            "Content extracted and saved for https://aniseblog.tw/201727\n",
            "Processing https://aniseblog.tw/201550\n",
            "Content extracted and saved for https://aniseblog.tw/201550\n",
            "Processing https://aniseblog.tw/200463\n",
            "Content extracted and saved for https://aniseblog.tw/200463\n",
            "Processing https://aniseblog.tw/198073\n",
            "Content extracted and saved for https://aniseblog.tw/198073\n",
            "Processing https://aniseblog.tw/195216\n",
            "Content extracted and saved for https://aniseblog.tw/195216\n",
            "Processing https://aniseblog.tw/194345\n",
            "Content extracted and saved for https://aniseblog.tw/194345\n",
            "Processing https://aniseblog.tw/194316\n",
            "Content extracted and saved for https://aniseblog.tw/194316\n",
            "Processing https://aniseblog.tw/193759\n",
            "Content extracted and saved for https://aniseblog.tw/193759\n",
            "Processing https://aniseblog.tw/192871\n",
            "Content extracted and saved for https://aniseblog.tw/192871\n",
            "Processing https://aniseblog.tw/192727\n",
            "Content extracted and saved for https://aniseblog.tw/192727\n",
            "Processing https://aniseblog.tw/192415\n",
            "Content extracted and saved for https://aniseblog.tw/192415\n",
            "Processing https://aniseblog.tw/192454\n",
            "Content extracted and saved for https://aniseblog.tw/192454\n",
            "Processing https://aniseblog.tw/192353\n",
            "Content extracted and saved for https://aniseblog.tw/192353\n",
            "Processing https://aniseblog.tw/192520\n",
            "Content extracted and saved for https://aniseblog.tw/192520\n",
            "Processing https://aniseblog.tw/192318\n",
            "Content extracted and saved for https://aniseblog.tw/192318\n",
            "Processing https://aniseblog.tw/192480\n",
            "Content extracted and saved for https://aniseblog.tw/192480\n",
            "Processing https://aniseblog.tw/192057\n",
            "Content extracted and saved for https://aniseblog.tw/192057\n",
            "Processing https://aniseblog.tw/191886\n",
            "Content extracted and saved for https://aniseblog.tw/191886\n",
            "Processing https://aniseblog.tw/191611\n",
            "Content extracted and saved for https://aniseblog.tw/191611\n",
            "Processing https://aniseblog.tw/191588\n",
            "Content extracted and saved for https://aniseblog.tw/191588\n",
            "Processing https://aniseblog.tw/191075\n",
            "Content extracted and saved for https://aniseblog.tw/191075\n",
            "Processing https://aniseblog.tw/190825\n",
            "Content extracted and saved for https://aniseblog.tw/190825\n",
            "Processing https://aniseblog.tw/190466\n",
            "Content extracted and saved for https://aniseblog.tw/190466\n",
            "Processing https://aniseblog.tw/190429\n",
            "Content extracted and saved for https://aniseblog.tw/190429\n",
            "Processing https://aniseblog.tw/190266\n",
            "Content extracted and saved for https://aniseblog.tw/190266\n",
            "Processing https://aniseblog.tw/190242\n",
            "Content extracted and saved for https://aniseblog.tw/190242\n",
            "Processing https://aniseblog.tw/190205\n",
            "Content extracted and saved for https://aniseblog.tw/190205\n",
            "Processing https://aniseblog.tw/189588\n",
            "Content extracted and saved for https://aniseblog.tw/189588\n",
            "Processing https://aniseblog.tw/189019\n",
            "Content extracted and saved for https://aniseblog.tw/189019\n",
            "Processing https://aniseblog.tw/188566\n",
            "Content extracted and saved for https://aniseblog.tw/188566\n",
            "Processing https://aniseblog.tw/188132\n",
            "Content extracted and saved for https://aniseblog.tw/188132\n",
            "Processing https://aniseblog.tw/186582\n",
            "Content extracted and saved for https://aniseblog.tw/186582\n",
            "Processing https://aniseblog.tw/185679\n",
            "Content extracted and saved for https://aniseblog.tw/185679\n",
            "Processing https://aniseblog.tw/182449\n",
            "Content extracted and saved for https://aniseblog.tw/182449\n",
            "Processing https://aniseblog.tw/181578\n",
            "Content extracted and saved for https://aniseblog.tw/181578\n",
            "Processing https://aniseblog.tw/181216\n",
            "Content extracted and saved for https://aniseblog.tw/181216\n",
            "Processing https://aniseblog.tw/180815\n",
            "Content extracted and saved for https://aniseblog.tw/180815\n",
            "Processing https://aniseblog.tw/180755\n",
            "Content extracted and saved for https://aniseblog.tw/180755\n",
            "Processing https://aniseblog.tw/179243\n",
            "Content extracted and saved for https://aniseblog.tw/179243\n",
            "Processing https://aniseblog.tw/179510\n",
            "Content extracted and saved for https://aniseblog.tw/179510\n",
            "Processing https://aniseblog.tw/178890\n",
            "Content extracted and saved for https://aniseblog.tw/178890\n",
            "Processing https://aniseblog.tw/177270\n",
            "Content extracted and saved for https://aniseblog.tw/177270\n",
            "Processing https://aniseblog.tw/176978\n",
            "Content extracted and saved for https://aniseblog.tw/176978\n",
            "Processing https://aniseblog.tw/173885\n",
            "Content extracted and saved for https://aniseblog.tw/173885\n",
            "Processing https://aniseblog.tw/173445\n",
            "Content extracted and saved for https://aniseblog.tw/173445\n",
            "Processing https://aniseblog.tw/171605\n",
            "Content extracted and saved for https://aniseblog.tw/171605\n",
            "Processing https://aniseblog.tw/170742\n",
            "Content extracted and saved for https://aniseblog.tw/170742\n",
            "Processing https://aniseblog.tw/170496\n",
            "Content extracted and saved for https://aniseblog.tw/170496\n",
            "Processing https://aniseblog.tw/166410\n",
            "Content extracted and saved for https://aniseblog.tw/166410\n",
            "Processing https://aniseblog.tw/166089\n",
            "Content extracted and saved for https://aniseblog.tw/166089\n",
            "Processing https://aniseblog.tw/164558\n",
            "Content extracted and saved for https://aniseblog.tw/164558\n",
            "Processing https://aniseblog.tw/150797\n",
            "Content extracted and saved for https://aniseblog.tw/150797\n",
            "Processing https://aniseblog.tw/148965\n",
            "Content extracted and saved for https://aniseblog.tw/148965\n",
            "Processing https://aniseblog.tw/148785\n",
            "Content extracted and saved for https://aniseblog.tw/148785\n",
            "Processing https://aniseblog.tw/148532\n",
            "Content extracted and saved for https://aniseblog.tw/148532\n",
            "Processing https://aniseblog.tw/104256\n",
            "Content extracted and saved for https://aniseblog.tw/104256\n",
            "Processing https://aniseblog.tw/114\n",
            "Content extracted and saved for https://aniseblog.tw/114\n",
            "Processing https://aniseblog.tw/150\n",
            "Content extracted and saved for https://aniseblog.tw/150\n",
            "Processing https://aniseblog.tw/282\n",
            "Content extracted and saved for https://aniseblog.tw/282\n",
            "Processing https://aniseblog.tw/441\n",
            "Content extracted and saved for https://aniseblog.tw/441\n",
            "Processing https://aniseblog.tw/768\n",
            "Content extracted and saved for https://aniseblog.tw/768\n",
            "Processing https://aniseblog.tw/769\n",
            "Content extracted and saved for https://aniseblog.tw/769\n",
            "Processing https://aniseblog.tw/786\n",
            "Content extracted and saved for https://aniseblog.tw/786\n",
            "Processing https://aniseblog.tw/897\n",
            "Content extracted and saved for https://aniseblog.tw/897\n",
            "Processing https://aniseblog.tw/1498\n",
            "Content extracted and saved for https://aniseblog.tw/1498\n",
            "Processing https://aniseblog.tw/1535\n",
            "Content extracted and saved for https://aniseblog.tw/1535\n",
            "Processing https://aniseblog.tw/1551\n",
            "Content extracted and saved for https://aniseblog.tw/1551\n",
            "Processing https://aniseblog.tw/2092\n",
            "Content extracted and saved for https://aniseblog.tw/2092\n",
            "Processing https://aniseblog.tw/2099\n",
            "Content extracted and saved for https://aniseblog.tw/2099\n",
            "Processing https://aniseblog.tw/2106\n",
            "Content extracted and saved for https://aniseblog.tw/2106\n",
            "Processing https://aniseblog.tw/2122\n",
            "Content extracted and saved for https://aniseblog.tw/2122\n",
            "Processing https://aniseblog.tw/2125\n",
            "Content extracted and saved for https://aniseblog.tw/2125\n",
            "Processing https://aniseblog.tw/2157\n",
            "Content extracted and saved for https://aniseblog.tw/2157\n",
            "Processing https://aniseblog.tw/2256\n",
            "Content extracted and saved for https://aniseblog.tw/2256\n",
            "Processing https://aniseblog.tw/2319\n",
            "Content extracted and saved for https://aniseblog.tw/2319\n",
            "Processing https://aniseblog.tw/2337\n",
            "Content extracted and saved for https://aniseblog.tw/2337\n",
            "Processing https://aniseblog.tw/2436\n",
            "Content extracted and saved for https://aniseblog.tw/2436\n",
            "Processing https://aniseblog.tw/2556\n",
            "Content extracted and saved for https://aniseblog.tw/2556\n",
            "Processing https://aniseblog.tw/2559\n",
            "Content extracted and saved for https://aniseblog.tw/2559\n",
            "Processing https://aniseblog.tw/2562\n",
            "Content extracted and saved for https://aniseblog.tw/2562\n",
            "Processing https://aniseblog.tw/2565\n",
            "Content extracted and saved for https://aniseblog.tw/2565\n",
            "Processing https://aniseblog.tw/2570\n",
            "Content extracted and saved for https://aniseblog.tw/2570\n",
            "Processing https://aniseblog.tw/2574\n",
            "Content extracted and saved for https://aniseblog.tw/2574\n",
            "Processing https://aniseblog.tw/2579\n",
            "Content extracted and saved for https://aniseblog.tw/2579\n",
            "Processing https://aniseblog.tw/2663\n",
            "Content extracted and saved for https://aniseblog.tw/2663\n",
            "Processing https://aniseblog.tw/2684\n",
            "Content extracted and saved for https://aniseblog.tw/2684\n",
            "Processing https://aniseblog.tw/2964\n",
            "Content extracted and saved for https://aniseblog.tw/2964\n",
            "Processing https://aniseblog.tw/2982\n",
            "Content extracted and saved for https://aniseblog.tw/2982\n",
            "Processing https://aniseblog.tw/2983\n",
            "Content extracted and saved for https://aniseblog.tw/2983\n",
            "Processing https://aniseblog.tw/2987\n",
            "Content extracted and saved for https://aniseblog.tw/2987\n",
            "Processing https://aniseblog.tw/3030\n",
            "Content extracted and saved for https://aniseblog.tw/3030\n",
            "Processing https://aniseblog.tw/3043\n",
            "Content extracted and saved for https://aniseblog.tw/3043\n",
            "Processing https://aniseblog.tw/3099\n",
            "Content extracted and saved for https://aniseblog.tw/3099\n",
            "Processing https://aniseblog.tw/3105\n",
            "Content extracted and saved for https://aniseblog.tw/3105\n",
            "Processing https://aniseblog.tw/3114\n",
            "Content extracted and saved for https://aniseblog.tw/3114\n",
            "Processing https://aniseblog.tw/3126\n",
            "Content extracted and saved for https://aniseblog.tw/3126\n",
            "Processing https://aniseblog.tw/3130\n",
            "Content extracted and saved for https://aniseblog.tw/3130\n",
            "Processing https://aniseblog.tw/3273\n",
            "Content extracted and saved for https://aniseblog.tw/3273\n",
            "Processing https://aniseblog.tw/3285\n",
            "Content extracted and saved for https://aniseblog.tw/3285\n",
            "Processing https://aniseblog.tw/3372\n",
            "Content extracted and saved for https://aniseblog.tw/3372\n",
            "Processing https://aniseblog.tw/3397\n",
            "Content extracted and saved for https://aniseblog.tw/3397\n",
            "Processing https://aniseblog.tw/3402\n",
            "Content extracted and saved for https://aniseblog.tw/3402\n",
            "Processing https://aniseblog.tw/3420\n",
            "Content extracted and saved for https://aniseblog.tw/3420\n",
            "Processing https://aniseblog.tw/3532\n",
            "Content extracted and saved for https://aniseblog.tw/3532\n",
            "Processing https://aniseblog.tw/3555\n",
            "Content extracted and saved for https://aniseblog.tw/3555\n",
            "Processing https://aniseblog.tw/3570\n",
            "Content extracted and saved for https://aniseblog.tw/3570\n",
            "Processing https://aniseblog.tw/3636\n",
            "Content extracted and saved for https://aniseblog.tw/3636\n",
            "Processing https://aniseblog.tw/3656\n",
            "Content extracted and saved for https://aniseblog.tw/3656\n",
            "Processing https://aniseblog.tw/3711\n",
            "Content extracted and saved for https://aniseblog.tw/3711\n",
            "Processing https://aniseblog.tw/3729\n",
            "Content extracted and saved for https://aniseblog.tw/3729\n",
            "Processing https://aniseblog.tw/3768\n",
            "Content extracted and saved for https://aniseblog.tw/3768\n",
            "Processing https://aniseblog.tw/3834\n",
            "Content extracted and saved for https://aniseblog.tw/3834\n",
            "Processing https://aniseblog.tw/3915\n",
            "Content extracted and saved for https://aniseblog.tw/3915\n",
            "Processing https://aniseblog.tw/3934\n",
            "Content extracted and saved for https://aniseblog.tw/3934\n",
            "Processing https://aniseblog.tw/3969\n",
            "Content extracted and saved for https://aniseblog.tw/3969\n",
            "Processing https://aniseblog.tw/3990\n",
            "Content extracted and saved for https://aniseblog.tw/3990\n",
            "Processing https://aniseblog.tw/3996\n",
            "Content extracted and saved for https://aniseblog.tw/3996\n",
            "Processing https://aniseblog.tw/4123\n",
            "Content extracted and saved for https://aniseblog.tw/4123\n",
            "Processing https://aniseblog.tw/4200\n",
            "Content extracted and saved for https://aniseblog.tw/4200\n",
            "Processing https://aniseblog.tw/4208\n",
            "Content extracted and saved for https://aniseblog.tw/4208\n",
            "Processing https://aniseblog.tw/4233\n",
            "Content extracted and saved for https://aniseblog.tw/4233\n",
            "Processing https://aniseblog.tw/4281\n",
            "Content extracted and saved for https://aniseblog.tw/4281\n",
            "Processing https://aniseblog.tw/4293\n",
            "Content extracted and saved for https://aniseblog.tw/4293\n",
            "Processing https://aniseblog.tw/4310\n",
            "Content extracted and saved for https://aniseblog.tw/4310\n",
            "Processing https://aniseblog.tw/4317\n",
            "Content extracted and saved for https://aniseblog.tw/4317\n",
            "Processing https://aniseblog.tw/4326\n",
            "Content extracted and saved for https://aniseblog.tw/4326\n",
            "Processing https://aniseblog.tw/4335\n",
            "Content extracted and saved for https://aniseblog.tw/4335\n",
            "Processing https://aniseblog.tw/4345\n",
            "Content extracted and saved for https://aniseblog.tw/4345\n",
            "Processing https://aniseblog.tw/4353\n",
            "Content extracted and saved for https://aniseblog.tw/4353\n",
            "Processing https://aniseblog.tw/4374\n",
            "Content extracted and saved for https://aniseblog.tw/4374\n",
            "Processing https://aniseblog.tw/4383\n",
            "Content extracted and saved for https://aniseblog.tw/4383\n",
            "Processing https://aniseblog.tw/4400\n",
            "Content extracted and saved for https://aniseblog.tw/4400\n",
            "Processing https://aniseblog.tw/4410\n",
            "Content extracted and saved for https://aniseblog.tw/4410\n",
            "Processing https://aniseblog.tw/4434\n",
            "Content extracted and saved for https://aniseblog.tw/4434\n",
            "Processing https://aniseblog.tw/4469\n",
            "Content extracted and saved for https://aniseblog.tw/4469\n",
            "Processing https://aniseblog.tw/4587\n",
            "Content extracted and saved for https://aniseblog.tw/4587\n",
            "Processing https://aniseblog.tw/4608\n",
            "Content extracted and saved for https://aniseblog.tw/4608\n",
            "Processing https://aniseblog.tw/4676\n",
            "Content extracted and saved for https://aniseblog.tw/4676\n",
            "Processing https://aniseblog.tw/4695\n",
            "Content extracted and saved for https://aniseblog.tw/4695\n",
            "Processing https://aniseblog.tw/4704\n",
            "Content extracted and saved for https://aniseblog.tw/4704\n",
            "Processing https://aniseblog.tw/4715\n",
            "Content extracted and saved for https://aniseblog.tw/4715\n",
            "Processing https://aniseblog.tw/4718\n",
            "Content extracted and saved for https://aniseblog.tw/4718\n",
            "Processing https://aniseblog.tw/4861\n",
            "Content extracted and saved for https://aniseblog.tw/4861\n",
            "Processing https://aniseblog.tw/5240\n",
            "Content extracted and saved for https://aniseblog.tw/5240\n",
            "Processing https://aniseblog.tw/5253\n",
            "Content extracted and saved for https://aniseblog.tw/5253\n",
            "Processing https://aniseblog.tw/5262\n",
            "Content extracted and saved for https://aniseblog.tw/5262\n",
            "Processing https://aniseblog.tw/5277\n",
            "Content extracted and saved for https://aniseblog.tw/5277\n",
            "Processing https://aniseblog.tw/5280\n",
            "Content extracted and saved for https://aniseblog.tw/5280\n",
            "Processing https://aniseblog.tw/5295\n",
            "Content extracted and saved for https://aniseblog.tw/5295\n",
            "Processing https://aniseblog.tw/5313\n",
            "Content extracted and saved for https://aniseblog.tw/5313\n",
            "Processing https://aniseblog.tw/5343\n",
            "Content extracted and saved for https://aniseblog.tw/5343\n",
            "Processing https://aniseblog.tw/5466\n",
            "Content extracted and saved for https://aniseblog.tw/5466\n",
            "Processing https://aniseblog.tw/5490\n",
            "Content extracted and saved for https://aniseblog.tw/5490\n",
            "Processing https://aniseblog.tw/5514\n",
            "Content extracted and saved for https://aniseblog.tw/5514\n",
            "Processing https://aniseblog.tw/5523\n",
            "Content extracted and saved for https://aniseblog.tw/5523\n",
            "Processing https://aniseblog.tw/5528\n",
            "Content extracted and saved for https://aniseblog.tw/5528\n",
            "Processing https://aniseblog.tw/5535\n",
            "Content extracted and saved for https://aniseblog.tw/5535\n",
            "Processing https://aniseblog.tw/5556\n",
            "Content extracted and saved for https://aniseblog.tw/5556\n",
            "Processing https://aniseblog.tw/5595\n",
            "Content extracted and saved for https://aniseblog.tw/5595\n",
            "Processing https://aniseblog.tw/5637\n",
            "Content extracted and saved for https://aniseblog.tw/5637\n",
            "Processing https://aniseblog.tw/5643\n",
            "Content extracted and saved for https://aniseblog.tw/5643\n",
            "Processing https://aniseblog.tw/5655\n",
            "Content extracted and saved for https://aniseblog.tw/5655\n",
            "Processing https://aniseblog.tw/5657\n",
            "Content extracted and saved for https://aniseblog.tw/5657\n",
            "Processing https://aniseblog.tw/5664\n",
            "Content extracted and saved for https://aniseblog.tw/5664\n",
            "Processing https://aniseblog.tw/5694\n",
            "Content extracted and saved for https://aniseblog.tw/5694\n",
            "Processing https://aniseblog.tw/5715\n",
            "Content extracted and saved for https://aniseblog.tw/5715\n",
            "Processing https://aniseblog.tw/5721\n",
            "Content extracted and saved for https://aniseblog.tw/5721\n",
            "Processing https://aniseblog.tw/5724\n",
            "Content extracted and saved for https://aniseblog.tw/5724\n",
            "Processing https://aniseblog.tw/5997\n",
            "Content extracted and saved for https://aniseblog.tw/5997\n",
            "Processing https://aniseblog.tw/6033\n",
            "Content extracted and saved for https://aniseblog.tw/6033\n",
            "Processing https://aniseblog.tw/6075\n",
            "Content extracted and saved for https://aniseblog.tw/6075\n",
            "Processing https://aniseblog.tw/6156\n",
            "Content extracted and saved for https://aniseblog.tw/6156\n",
            "Processing https://aniseblog.tw/6198\n",
            "Content extracted and saved for https://aniseblog.tw/6198\n",
            "Processing https://aniseblog.tw/6213\n",
            "Content extracted and saved for https://aniseblog.tw/6213\n",
            "Processing https://aniseblog.tw/6288\n",
            "Content extracted and saved for https://aniseblog.tw/6288\n",
            "Processing https://aniseblog.tw/6345\n",
            "Content extracted and saved for https://aniseblog.tw/6345\n",
            "Processing https://aniseblog.tw/6366\n",
            "Content extracted and saved for https://aniseblog.tw/6366\n",
            "Processing https://aniseblog.tw/6384\n",
            "Content extracted and saved for https://aniseblog.tw/6384\n",
            "Processing https://aniseblog.tw/6390\n",
            "Content extracted and saved for https://aniseblog.tw/6390\n",
            "Processing https://aniseblog.tw/6393\n",
            "Content extracted and saved for https://aniseblog.tw/6393\n",
            "Processing https://aniseblog.tw/6399\n",
            "Content extracted and saved for https://aniseblog.tw/6399\n",
            "Processing https://aniseblog.tw/6414\n",
            "Content extracted and saved for https://aniseblog.tw/6414\n",
            "Processing https://aniseblog.tw/6426\n",
            "Content extracted and saved for https://aniseblog.tw/6426\n",
            "Processing https://aniseblog.tw/6433\n",
            "Content extracted and saved for https://aniseblog.tw/6433\n",
            "Processing https://aniseblog.tw/6450\n",
            "Content extracted and saved for https://aniseblog.tw/6450\n",
            "Processing https://aniseblog.tw/6462\n",
            "Content extracted and saved for https://aniseblog.tw/6462\n",
            "Processing https://aniseblog.tw/6477\n",
            "Content extracted and saved for https://aniseblog.tw/6477\n",
            "Processing https://aniseblog.tw/6486\n",
            "Content extracted and saved for https://aniseblog.tw/6486\n",
            "Processing https://aniseblog.tw/6510\n",
            "Content extracted and saved for https://aniseblog.tw/6510\n",
            "Processing https://aniseblog.tw/6516\n",
            "Content extracted and saved for https://aniseblog.tw/6516\n",
            "Processing https://aniseblog.tw/6531\n",
            "Content extracted and saved for https://aniseblog.tw/6531\n",
            "Processing https://aniseblog.tw/6532\n",
            "Content extracted and saved for https://aniseblog.tw/6532\n",
            "Processing https://aniseblog.tw/6544\n",
            "Content extracted and saved for https://aniseblog.tw/6544\n",
            "Processing https://aniseblog.tw/6568\n",
            "Content extracted and saved for https://aniseblog.tw/6568\n",
            "Processing https://aniseblog.tw/6574\n",
            "Content extracted and saved for https://aniseblog.tw/6574\n",
            "Processing https://aniseblog.tw/6616\n",
            "Content extracted and saved for https://aniseblog.tw/6616\n",
            "Processing https://aniseblog.tw/6640\n",
            "Content extracted and saved for https://aniseblog.tw/6640\n",
            "Processing https://aniseblog.tw/6646\n",
            "Content extracted and saved for https://aniseblog.tw/6646\n",
            "Processing https://aniseblog.tw/6664\n",
            "Content extracted and saved for https://aniseblog.tw/6664\n",
            "Processing https://aniseblog.tw/6667\n",
            "Content extracted and saved for https://aniseblog.tw/6667\n",
            "Processing https://aniseblog.tw/6670\n",
            "Content extracted and saved for https://aniseblog.tw/6670\n",
            "Processing https://aniseblog.tw/6694\n",
            "Content extracted and saved for https://aniseblog.tw/6694\n",
            "Processing https://aniseblog.tw/6709\n",
            "Content extracted and saved for https://aniseblog.tw/6709\n",
            "Processing https://aniseblog.tw/6715\n",
            "Content extracted and saved for https://aniseblog.tw/6715\n",
            "Processing https://aniseblog.tw/6721\n",
            "Content extracted and saved for https://aniseblog.tw/6721\n",
            "Processing https://aniseblog.tw/6730\n",
            "Content extracted and saved for https://aniseblog.tw/6730\n",
            "Processing https://aniseblog.tw/6733\n",
            "Content extracted and saved for https://aniseblog.tw/6733\n",
            "Processing https://aniseblog.tw/6736\n",
            "Content extracted and saved for https://aniseblog.tw/6736\n",
            "Processing https://aniseblog.tw/6754\n",
            "Content extracted and saved for https://aniseblog.tw/6754\n",
            "Processing https://aniseblog.tw/6766\n",
            "Content extracted and saved for https://aniseblog.tw/6766\n",
            "Processing https://aniseblog.tw/6772\n",
            "Content extracted and saved for https://aniseblog.tw/6772\n",
            "Processing https://aniseblog.tw/6784\n",
            "Content extracted and saved for https://aniseblog.tw/6784\n",
            "Processing https://aniseblog.tw/6796\n",
            "Content extracted and saved for https://aniseblog.tw/6796\n",
            "Processing https://aniseblog.tw/6817\n",
            "Content extracted and saved for https://aniseblog.tw/6817\n",
            "Processing https://aniseblog.tw/6826\n",
            "Content extracted and saved for https://aniseblog.tw/6826\n",
            "Processing https://aniseblog.tw/6832\n",
            "Content extracted and saved for https://aniseblog.tw/6832\n",
            "Processing https://aniseblog.tw/6838\n",
            "Content extracted and saved for https://aniseblog.tw/6838\n",
            "Processing https://aniseblog.tw/6874\n",
            "Content extracted and saved for https://aniseblog.tw/6874\n",
            "Processing https://aniseblog.tw/6886\n",
            "Content extracted and saved for https://aniseblog.tw/6886\n",
            "Processing https://aniseblog.tw/6892\n",
            "Content extracted and saved for https://aniseblog.tw/6892\n",
            "Processing https://aniseblog.tw/6898\n",
            "Content extracted and saved for https://aniseblog.tw/6898\n",
            "Processing https://aniseblog.tw/6907\n",
            "Content extracted and saved for https://aniseblog.tw/6907\n",
            "Processing https://aniseblog.tw/6928\n",
            "Content extracted and saved for https://aniseblog.tw/6928\n",
            "Processing https://aniseblog.tw/6934\n",
            "Content extracted and saved for https://aniseblog.tw/6934\n",
            "Processing https://aniseblog.tw/6937\n",
            "Content extracted and saved for https://aniseblog.tw/6937\n",
            "Processing https://aniseblog.tw/6940\n",
            "Content extracted and saved for https://aniseblog.tw/6940\n",
            "Processing https://aniseblog.tw/6961\n",
            "Content extracted and saved for https://aniseblog.tw/6961\n",
            "Processing https://aniseblog.tw/6997\n",
            "Content extracted and saved for https://aniseblog.tw/6997\n",
            "Processing https://aniseblog.tw/7036\n",
            "Content extracted and saved for https://aniseblog.tw/7036\n",
            "Processing https://aniseblog.tw/7039\n",
            "Content extracted and saved for https://aniseblog.tw/7039\n",
            "Processing https://aniseblog.tw/7111\n",
            "Content extracted and saved for https://aniseblog.tw/7111\n",
            "Processing https://aniseblog.tw/7120\n",
            "Content extracted and saved for https://aniseblog.tw/7120\n",
            "Processing https://aniseblog.tw/7126\n",
            "Content extracted and saved for https://aniseblog.tw/7126\n",
            "Processing https://aniseblog.tw/7138\n",
            "Content extracted and saved for https://aniseblog.tw/7138\n",
            "Processing https://aniseblog.tw/7147\n",
            "Content extracted and saved for https://aniseblog.tw/7147\n",
            "Processing https://aniseblog.tw/7159\n",
            "Content extracted and saved for https://aniseblog.tw/7159\n",
            "Processing https://aniseblog.tw/7201\n",
            "Content extracted and saved for https://aniseblog.tw/7201\n",
            "Processing https://aniseblog.tw/7204\n",
            "Content extracted and saved for https://aniseblog.tw/7204\n",
            "Processing https://aniseblog.tw/7207\n",
            "Content extracted and saved for https://aniseblog.tw/7207\n",
            "Processing https://aniseblog.tw/7210\n",
            "Content extracted and saved for https://aniseblog.tw/7210\n",
            "Processing https://aniseblog.tw/7213\n",
            "Content extracted and saved for https://aniseblog.tw/7213\n",
            "Processing https://aniseblog.tw/7216\n",
            "Content extracted and saved for https://aniseblog.tw/7216\n",
            "Processing https://aniseblog.tw/7222\n",
            "Content extracted and saved for https://aniseblog.tw/7222\n",
            "Processing https://aniseblog.tw/7313\n",
            "Content extracted and saved for https://aniseblog.tw/7313\n",
            "Processing https://aniseblog.tw/7326\n",
            "Content extracted and saved for https://aniseblog.tw/7326\n",
            "Processing https://aniseblog.tw/7343\n",
            "Content extracted and saved for https://aniseblog.tw/7343\n",
            "Processing https://aniseblog.tw/7349\n",
            "Content extracted and saved for https://aniseblog.tw/7349\n",
            "Processing https://aniseblog.tw/7358\n",
            "Content extracted and saved for https://aniseblog.tw/7358\n",
            "Processing https://aniseblog.tw/7397\n",
            "Content extracted and saved for https://aniseblog.tw/7397\n",
            "Processing https://aniseblog.tw/7427\n",
            "Content extracted and saved for https://aniseblog.tw/7427\n",
            "Processing https://aniseblog.tw/7439\n",
            "Content extracted and saved for https://aniseblog.tw/7439\n",
            "Processing https://aniseblog.tw/7496\n",
            "Content extracted and saved for https://aniseblog.tw/7496\n",
            "Processing https://aniseblog.tw/7514\n",
            "Content extracted and saved for https://aniseblog.tw/7514\n",
            "Processing https://aniseblog.tw/7531\n",
            "Content extracted and saved for https://aniseblog.tw/7531\n",
            "Processing https://aniseblog.tw/7538\n",
            "Content extracted and saved for https://aniseblog.tw/7538\n",
            "Processing https://aniseblog.tw/7571\n",
            "Content extracted and saved for https://aniseblog.tw/7571\n",
            "Processing https://aniseblog.tw/7574\n",
            "Content extracted and saved for https://aniseblog.tw/7574\n",
            "Processing https://aniseblog.tw/7577\n",
            "Content extracted and saved for https://aniseblog.tw/7577\n",
            "Processing https://aniseblog.tw/7583\n",
            "Content extracted and saved for https://aniseblog.tw/7583\n",
            "Processing https://aniseblog.tw/7595\n",
            "Content extracted and saved for https://aniseblog.tw/7595\n",
            "Processing https://aniseblog.tw/7604\n",
            "Content extracted and saved for https://aniseblog.tw/7604\n",
            "Processing https://aniseblog.tw/7610\n",
            "Content extracted and saved for https://aniseblog.tw/7610\n",
            "Processing https://aniseblog.tw/7625\n",
            "Content extracted and saved for https://aniseblog.tw/7625\n",
            "Processing https://aniseblog.tw/7646\n",
            "Content extracted and saved for https://aniseblog.tw/7646\n",
            "Processing https://aniseblog.tw/7688\n",
            "Content extracted and saved for https://aniseblog.tw/7688\n",
            "Processing https://aniseblog.tw/7715\n",
            "Content extracted and saved for https://aniseblog.tw/7715\n",
            "Processing https://aniseblog.tw/7724\n",
            "Content extracted and saved for https://aniseblog.tw/7724\n",
            "Processing https://aniseblog.tw/7745\n",
            "Content extracted and saved for https://aniseblog.tw/7745\n",
            "Processing https://aniseblog.tw/7754\n",
            "Content extracted and saved for https://aniseblog.tw/7754\n",
            "Processing https://aniseblog.tw/7763\n",
            "Content extracted and saved for https://aniseblog.tw/7763\n",
            "Processing https://aniseblog.tw/7774\n",
            "Content extracted and saved for https://aniseblog.tw/7774\n",
            "Processing https://aniseblog.tw/7802\n",
            "Content extracted and saved for https://aniseblog.tw/7802\n",
            "Processing https://aniseblog.tw/7823\n",
            "Content extracted and saved for https://aniseblog.tw/7823\n",
            "Processing https://aniseblog.tw/7829\n",
            "Content extracted and saved for https://aniseblog.tw/7829\n",
            "Processing https://aniseblog.tw/7835\n",
            "Content extracted and saved for https://aniseblog.tw/7835\n",
            "Processing https://aniseblog.tw/7838\n",
            "Content extracted and saved for https://aniseblog.tw/7838\n",
            "Processing https://aniseblog.tw/7853\n",
            "Content extracted and saved for https://aniseblog.tw/7853\n",
            "Processing https://aniseblog.tw/7883\n",
            "Content extracted and saved for https://aniseblog.tw/7883\n",
            "Processing https://aniseblog.tw/7934\n",
            "Content extracted and saved for https://aniseblog.tw/7934\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# Base URL of the blog\n",
        "base_url = 'https://aniseblog.tw/categories/%E5%8F%B0%E7%81%A3%E6%97%85%E9%81%8A/'\n",
        "\n",
        "# Function to fetch the HTML content of a page\n",
        "def fetch_page(url):\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        return response.content\n",
        "    else:\n",
        "        print(f\"Failed to retrieve {url}. Status code: {response.status_code}\")\n",
        "        return None\n",
        "\n",
        "# Function to extract all blog post URLs from a page\n",
        "def extract_post_urls(page_content):\n",
        "    soup = BeautifulSoup(page_content, 'html.parser')\n",
        "    # Use an attribute selector to find all <a> tags with rel=\"bookmark\"\n",
        "    post_links = soup.select('.entry-title a[rel=\"bookmark\"]')\n",
        "    post_urls = [urljoin(base_url, link['href']) for link in post_links]\n",
        "    return post_urls\n",
        "\n",
        "# Function to extract the main content from a blog post page\n",
        "def extract_post_content(post_page_content):\n",
        "    soup = BeautifulSoup(post_page_content, 'html.parser')\n",
        "    main_content_div = soup.find('div', class_='entry-content')\n",
        "    if main_content_div:\n",
        "        return main_content_div.get_text(separator=' ', strip=True)\n",
        "    else:\n",
        "        print(\"Main content div not found.\")\n",
        "        return ''\n",
        "\n",
        "# Main script\n",
        "def main():\n",
        "    all_post_urls = []\n",
        "    num_pages = 50  # Total number of pages\n",
        "\n",
        "    # Loop through all pages to collect blog post URLs\n",
        "    for page_num in range(1, num_pages + 1):\n",
        "        page_url = f\"{base_url}page/{page_num}/\"\n",
        "        print(f\"Fetching page {page_num}: {page_url}\")\n",
        "        page_content = fetch_page(page_url)\n",
        "        if page_content:\n",
        "            post_urls = extract_post_urls(page_content)\n",
        "            all_post_urls.extend(post_urls)\n",
        "\n",
        "    # Open the file to save the blog posts content\n",
        "    with open('tw_posts.txt', 'w', encoding='utf-8') as file:\n",
        "        for url in all_post_urls:\n",
        "            print(f\"Processing {url}\")\n",
        "            post_page_content = fetch_page(url)\n",
        "            if post_page_content:\n",
        "                post_content = extract_post_content(post_page_content)\n",
        "                file.write(f\"URL: {url}\\n\")\n",
        "                file.write(post_content + \"\\n\\n\")  # Add a newline to separate each post's content\n",
        "                print(f\"Content extracted and saved for {url}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#讀取長文本\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextLoader\n\u001b[1;32m      4\u001b[0m loader \u001b[38;5;241m=\u001b[39m TextLoader(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./tw_posts.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m doc \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'"
          ]
        }
      ],
      "source": [
        "#讀取長文本\n",
        "from langchain.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader('./tw_posts.txt')\n",
        "doc = loader.load()\n",
        "print (f\"You have {len(doc)} document\")\n",
        "print (f\"You have {len(doc[0].page_content)} characters in that document\")\n",
        "\n",
        "#切分長文本\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=100)\n",
        "splitted_docs = text_splitter.split_documents(doc)\n",
        "\n",
        "\n",
        "num_total_characters = sum([len(x.page_content) for x in splitted_docs])\n",
        "\n",
        "print(f\"我們有 {len(splitted_docs)} splitted documents\")\n",
        "print(f\"平均每個 splitted document 有 {num_total_characters / len(splitted_docs):,.0f} characters\")\n",
        "\n",
        "# Get the embeddings engine ready\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "my_embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-l6-v2\",\n",
        "    api_key=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],\n",
        ")\n",
        "# The vectorstore we use: Chroma\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "my_vectorstore = Chroma(embedding_function = my_embeddings,\n",
        "                        persist_directory = './Chroma_DATABASE')\n",
        "# Embed the splitted documents and add it to my vectorstore\n",
        "\n",
        "my_vectorstore.add_documents(splitted_docs)\n",
        "\n",
        "# 定義 retriever\n",
        "# 要求他只返回最相似(最可能和query相關) 的 2 個 document\n",
        "retriever = my_vectorstore.as_retriever(search_kwargs={\"k\": 2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "## 定義 prompt\n",
        "template = \"\"\"根據給定 context 回答問題:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "## 定義模型\n",
        "llm = HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "                          max_length=128, temperature=0.5,)\n",
        "\n",
        "\n",
        "## 多加了一個整理 splitted doc 的小函數\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "\n",
        "## 定義 chain\n",
        "chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import gradio as gr\n",
        "\n",
        "\n",
        "def slow_echo(message, history):\n",
        "    chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    )\n",
        "    time.sleep(0.05)\n",
        "    user_query = message\n",
        "    ans = chain.invoke(user_query)\n",
        "    #print(ans)\n",
        "    yield ans[8:]\n",
        "\n",
        "\n",
        "demo = gr.ChatInterface(slow_echo).queue()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import gradio as gr\n",
        "\n",
        "# Updated slow_echo function\n",
        "def slow_echo(message, history=None):\n",
        "    chain = (\n",
        "        {\"context\": retriever(message) | format_docs, \"question\": RunnablePassthrough()}\n",
        "        | prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    time.sleep(0.05)\n",
        "    user_query = message\n",
        "    ans = chain(user_query)\n",
        "    return ans[8:]\n",
        "\n",
        "def sentence_builder(place, num, theme):\n",
        "    return f\"I would like to visit {' and '.join(place)}, and check out {num} cool places about {theme}. Please list {num} places.\"\n",
        "\n",
        "def build_sentence_and_echo(place, num, theme):\n",
        "    sentence = sentence_builder(place, num, theme)\n",
        "    processed_sentence = slow_echo(sentence)\n",
        "    return processed_sentence\n",
        "\n",
        "# Create the Gradio interface using Blocks\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            # Inputs for sentence_builder\n",
        "            place_input = gr.CheckboxGroup([\"Taipei\", \"Taichung\", \"Kaohsiung\"], label=\"Places\", info=\"Where do you wanna go?\")\n",
        "            num_input = gr.Slider(1, 5, value=3, step=1, label=\"Number of Attractions\", info=\"Choose between 1 and 5\")\n",
        "            theme_input = gr.Dropdown(\n",
        "                [\"movie theaters\", \"the mining industry\", \"shrines\"], \n",
        "                label=\"Theme\", \n",
        "                info=\"What kind of place do you wanna know about?\"\n",
        "            )\n",
        "            build_button = gr.Button(\"Build Sentence\")\n",
        "            \n",
        "            # Output of slow_echo\n",
        "            echo_output = gr.Textbox(label=\"Echo Output\")\n",
        "        \n",
        "        with gr.Column():\n",
        "            # ChatInterface for slow_echo\n",
        "            chatbot = gr.ChatInterface(slow_echo).queue()\n",
        "\n",
        "    # Link the build button to the combined function\n",
        "    build_button.click(\n",
        "        build_sentence_and_echo,\n",
        "        inputs=[place_input, num_input, theme_input],\n",
        "        outputs=echo_output\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 建一個有其他選項的gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read API Key\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# 失敗的話會列印出：'沒讀到 HUGGINGFACEHUB_API key'\n",
        "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN', '沒讀到 HUGGINGFACEHUB_API_TOKEN')\n",
        "GRAPHHOPPER_API_TOKEN = os.getenv('GRAPHHOPPER_API_TOKEN', '沒讀到 GRAPHHOPPER_API_TOKEN')\n",
        "\n",
        "print(GRAPHHOPPER_API_TOKEN)\n",
        "print(HUGGINGFACEHUB_API_TOKEN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import gradio as gr\n",
        "\n",
        "def slow_echo(message, history):\n",
        "    chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        "    )\n",
        "    time.sleep(0.05)\n",
        "    user_query = message\n",
        "    ans = chain.invoke(user_query)\n",
        "    #print(ans)\n",
        "    yield ans[8:]\n",
        "\n",
        "def sentence_builder(place, num, theme):\n",
        "    return f\"I would like to visit {' and '.join(place)}, and check out {num} cool places about {theme}. Please list {num} places.\"\n",
        "\n",
        "demo = gr.Interface(sentence_builder,\n",
        "    [\n",
        "        gr.CheckboxGroup([\"Taipei\", \"Taichung\", \"Kaohsiung\"], label=\"Places\", info=\"Where do you wanna go?\"),\n",
        "        gr.Slider(1, 5, value=3, step=1, label=\"Number of Attractions\", info=\"Choose between 1 and 5\"),\n",
        "        gr.Dropdown(\n",
        "            [\"movie theaters\", \"the mining industry\", \"shrines\"], label=\"Theme\", info=\"What kind of place do you wanna know about?\"\n",
        "        )\n",
        "    ],\n",
        "    \"text\",\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import gradio as gr\n",
        "\n",
        "def slow_echo(message, history=None):\n",
        "    time.sleep(0.05)\n",
        "    response = llm(message)\n",
        "    return response\n",
        "\n",
        "def sentence_builder(place, num, theme):\n",
        "    return f\"I would like to visit {' and '.join(place)}, and check out {num} cool places about {theme}. Please list {num} places.\"\n",
        "\n",
        "def build_sentence_and_echo(place, num, theme):\n",
        "    sentence = sentence_builder(place, num, theme)\n",
        "    processed_sentence = slow_echo(sentence)\n",
        "    return sentence, processed_sentence\n",
        "\n",
        "# Create the Gradio interface using Blocks\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            # Inputs for sentence_builder\n",
        "            place_input = gr.CheckboxGroup([\"Taipei\", \"Taichung\", \"Kaohsiung\"], label=\"Places\", info=\"Where do you wanna go?\")\n",
        "            num_input = gr.Slider(1, 5, value=3, step=1, label=\"Number of Attractions\", info=\"Choose between 1 and 5\")\n",
        "            theme_input = gr.Dropdown(\n",
        "                [\"movie theaters\", \"the mining industry\", \"shrines\"], \n",
        "                label=\"Theme\", \n",
        "                info=\"What kind of place do you wanna know about?\"\n",
        "            )\n",
        "            build_button = gr.Button(\"Build Sentence\")\n",
        "            \n",
        "            # Output of sentence_builder and slow_echo\n",
        "            sentence_output = gr.Textbox(label=\"Sentence Output\")\n",
        "            echo_output = gr.Textbox(label=\"LLM Output\")\n",
        "        \n",
        "        with gr.Column():\n",
        "            # ChatInterface for slow_echo\n",
        "            chatbot = gr.ChatInterface(slow_echo).queue()\n",
        "\n",
        "    # Link the build button to the combined function\n",
        "    build_button.click(\n",
        "        build_sentence_and_echo,\n",
        "        inputs=[place_input, num_input, theme_input],\n",
        "        outputs=[sentence_output, echo_output]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Okay I think this works! Run below this line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install -U -q langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You have 1 document\n",
            "You have 1506520 characters in that document\n",
            "我們有 920 splitted documents\n",
            "平均每個 splitted document 有 1,658 characters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING! max_length is not default parameter.\n",
            "                    max_length was transferred to model_kwargs.\n",
            "                    Please make sure that max_length is what you intended.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /Users/user/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "#讀取長文本\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "import os\n",
        "#from langchain_huggingface import llms \n",
        "#import HuggingFaceEndpoint\n",
        "\n",
        "loader = TextLoader('./tw_posts.txt')\n",
        "doc = loader.load()\n",
        "print (f\"You have {len(doc)} document\")\n",
        "print (f\"You have {len(doc[0].page_content)} characters in that document\")\n",
        "\n",
        "#切分長文本\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=100)\n",
        "splitted_docs = text_splitter.split_documents(doc)\n",
        "\n",
        "\n",
        "num_total_characters = sum([len(x.page_content) for x in splitted_docs])\n",
        "\n",
        "print(f\"我們有 {len(splitted_docs)} splitted documents\")\n",
        "print(f\"平均每個 splitted document 有 {num_total_characters / len(splitted_docs):,.0f} characters\")\n",
        "\n",
        "# Get the embeddings engine ready\n",
        "my_embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-l6-v2\",\n",
        "    api_key=os.environ[\"HUGGINGFACEHUB_API_TOKEN\"],\n",
        ")\n",
        "# The vectorstore we use: Chroma\n",
        "my_vectorstore = Chroma(embedding_function = my_embeddings,\n",
        "                        persist_directory = './Chroma_DATABASE')\n",
        "# Embed the splitted documents and add it to my vectorstore\n",
        "\n",
        "my_vectorstore.add_documents(splitted_docs)\n",
        "\n",
        "# 定義 retriever\n",
        "# 要求他只返回最相似(最可能和query相關) 的 2 個 document\n",
        "retriever = my_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "## 定義 prompt\n",
        "template = \"\"\"Answer the question only based on the given context. If you don't know something, just say so. This is the context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "Please verify the answer. If the answer is false, regenerate the answer by providing the correct information. If the answer is true, please provide the source of the information.\n",
        "\"\"\"\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "\n",
        "## 定義模型\n",
        "llm = HuggingFaceEndpoint(repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
        "                          max_length=128, temperature=0.5,)\n",
        "\n",
        "\n",
        "## 多加了一個整理 splitted doc 的小函數\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "\n",
        "## 定義 chain\n",
        "chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7862\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import re\n",
        "import time\n",
        "import gradio as gr\n",
        "\n",
        "def slow_echo(message, history=None):\n",
        "    time.sleep(0.05)\n",
        "    response = llm(message)\n",
        "    return response\n",
        "\n",
        "def sentence_builder(place, num, theme):\n",
        "    return f\"I would like to visit {' and '.join(place)}, and check out {num} cool places about {theme}. Please list {num} places.\"\n",
        "\n",
        "def build_sentence_and_echo(place, num, theme):\n",
        "    sentence = sentence_builder(place, num, theme)\n",
        "    processed_sentence = slow_echo(sentence)\n",
        "    return sentence, processed_sentence\n",
        "\n",
        "def extract_place_names(response):\n",
        "    # Regex to match the place names in the format '1. Name, Location:'\n",
        "    matches = re.findall(r\"\\d+\\.\\s*(.*?):\", response)\n",
        "    # Join the extracted names with a comma and return\n",
        "    place_names = ', '.join(matches)\n",
        "    return place_names\n",
        "\n",
        "\n",
        "def clear_outputs():\n",
        "    return \"\", \"\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            place_input = gr.CheckboxGroup([\"基隆\", \"台北\", \"桃園\", \"新竹\", \"苗栗\", \"台中\", \"彰化\", \"南投\", \"雲林\", \"嘉義\", \"台南\", \"高雄\", \"屏東\", \"宜蘭\", \"花蓮\", \"台東\"], label=\"Places\", info=\"你想去哪？\")\n",
        "            num_input = gr.Slider(2, 5, value=3, step=1, label=\"去幾個地方？\", info=\"選2-5個\")\n",
        "            theme_input = gr.Dropdown(\n",
        "                [\"農場\", \"步道\", \"好吃的\", \"博物館\", \"宗教景點\", \"觀光工廠\", \"老街\"],\n",
        "                label=\"Theme\",\n",
        "                info=\"想去怎樣的地方呢？\"\n",
        "            )\n",
        "            build_button = gr.Button(\"快跟我講\")\n",
        "            sentence_output = gr.Textbox(label=\"我的需求\")\n",
        "            echo_output = gr.Textbox(label=\"LLM說\")\n",
        "            like_button = gr.Button(\"就去這裡！\")\n",
        "            dislike_button = gr.Button(\"砍掉重練\")\n",
        "            places_liked = gr.Textbox(label=\"喜歡的地方\")\n",
        "\n",
        "        with gr.Column():\n",
        "            chatbot = gr.ChatInterface(slow_echo).queue()\n",
        "\n",
        "    build_button.click(\n",
        "        build_sentence_and_echo,\n",
        "        inputs=[place_input, num_input, theme_input],\n",
        "        outputs=[sentence_output, echo_output]\n",
        "    )\n",
        "\n",
        "    like_button.click(\n",
        "        lambda response: extract_place_names(response),\n",
        "        inputs=echo_output,\n",
        "        outputs=places_liked\n",
        "    )\n",
        "\n",
        "    dislike_button.click(\n",
        "        clear_outputs,\n",
        "        outputs=[sentence_output, echo_output]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 來加地圖的東西"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7867\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import re\n",
        "import time\n",
        "import gradio as gr\n",
        "import os\n",
        "import googlemaps\n",
        "from datetime import datetime\n",
        "import folium\n",
        "from folium import IFrame\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import requests\n",
        "\n",
        "def slow_echo(message, history=None):\n",
        "    time.sleep(0.05)\n",
        "    response = llm(message)\n",
        "    return response\n",
        "\n",
        "def sentence_builder(place, num, theme):\n",
        "    return f\"我想到 {' 和 '.join(place)}， 然後去 {num} 個關於 {theme}的地方。 請列出 {num} 個這樣的地方。\"\n",
        "\n",
        "def build_sentence_and_echo(place, num, theme):\n",
        "    sentence = sentence_builder(place, num, theme)\n",
        "    processed_sentence = slow_echo(sentence)\n",
        "    return sentence, processed_sentence\n",
        "\n",
        "def extract_place_names(response):\n",
        "    # Regex to match the place names in the format '1. Name, Location:'\n",
        "    matches = re.findall(r\"\\d+\\.\\s*(.*?):\", response)\n",
        "    # Join the extracted names with a comma and return\n",
        "    place_names = ', '.join(matches)\n",
        "    return place_names\n",
        "\n",
        "def clear_outputs():\n",
        "    return \"\", \"\"\n",
        "\n",
        "#地圖的東西\n",
        "# Google Maps API Key\n",
        "gmaps = googlemaps.Client(key=os.environ[\"GOOGLE_MAPS_API_KEY\"])\n",
        "\n",
        "def generate_route_map(places):\n",
        "\n",
        "    # Check if places is a string and convert to list\n",
        "    if isinstance(places, str):\n",
        "      places = [place.strip() for place in places.split(\",\")]\n",
        "\n",
        "    # Get coordinates for each place\n",
        "    coordinates = []\n",
        "    for place in places:\n",
        "      print(place)\n",
        "      geocode_result = gmaps.geocode(place)\n",
        "      if geocode_result:\n",
        "          coordinates.append(geocode_result[0]['geometry']['location'])\n",
        "      else:\n",
        "          print(f\"Geocode not found for: {place}\")\n",
        "\n",
        "    if len(coordinates) < 2:\n",
        "        return \"Not enough places to create a route.\"\n",
        "\n",
        "    # Generate the best route\n",
        "    waypoints = coordinates[1:-1]\n",
        "    directions_result = gmaps.directions(coordinates[0], coordinates[-1], waypoints=waypoints, mode=\"driving\", departure_time=datetime.now())\n",
        "\n",
        "    if not directions_result:\n",
        "        return \"No route found for the given places.\"\n",
        "\n",
        "    # Create a map\n",
        "    route_map = folium.Map(location=[coordinates[0]['lat'], coordinates[0]['lng']], zoom_start=12)\n",
        "\n",
        "    # Add markers to the map\n",
        "    for i, coordinate in enumerate(coordinates):\n",
        "        folium.Marker([coordinate['lat'], coordinate['lng']], tooltip=places[i]).add_to(route_map)\n",
        "\n",
        "    # Add route to the map\n",
        "    route = directions_result[0]['legs']\n",
        "    for leg in route:\n",
        "        for step in leg['steps']:\n",
        "            start_loc = step['start_location']\n",
        "            end_loc = step['end_location']\n",
        "            folium.PolyLine(locations=[(start_loc['lat'], start_loc['lng']), (end_loc['lat'], end_loc['lng'])], color='blue', weight=5).add_to(route_map)\n",
        "\n",
        "    # Save map as HTML\n",
        "    map_path = '/content/route_map.html'  # Save in Colab's file system\n",
        "    route_map.save(map_path)\n",
        "\n",
        "    return map_path\n",
        "###\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            place_input = gr.CheckboxGroup([\"基隆\", \"台北\", \"桃園\", \"新竹\", \"苗栗\", \"台中\", \"彰化\", \"南投\", \"雲林\", \"嘉義\", \"台南\", \"高雄\", \"屏東\", \"宜蘭\", \"花蓮\", \"台東\"], label=\"Places\", info=\"你想去哪？\")\n",
        "            num_input = gr.Slider(2, 5, value=3, step=1, label=\"去幾個地方？\", info=\"選2-5個\")\n",
        "            theme_input = gr.Dropdown(\n",
        "                [\"農場\", \"步道\", \"好吃的\", \"博物館\", \"宗教景點\", \"觀光工廠\", \"老街\"],\n",
        "                label=\"Theme\",\n",
        "                info=\"想去怎樣的地方呢？\"\n",
        "            )\n",
        "            build_button = gr.Button(\"快跟我講\")\n",
        "            sentence_output = gr.Textbox(label=\"我的需求\")\n",
        "            echo_output = gr.Textbox(label=\"LLM說\")\n",
        "            like_button = gr.Button(\"就去這裡！\")\n",
        "            dislike_button = gr.Button(\"砍掉重練\")\n",
        "            places_liked = gr.Textbox(label=\"喜歡的地方\")\n",
        "            map_button = gr.Button(\"生成地圖\")\n",
        "            map_output = gr.File(label=\"點我看地圖\")\n",
        "\n",
        "        with gr.Column():\n",
        "            chatbot = gr.ChatInterface(slow_echo).queue()\n",
        "\n",
        "    build_button.click(\n",
        "        build_sentence_and_echo,\n",
        "        inputs=[place_input, num_input, theme_input],\n",
        "        outputs=[sentence_output, echo_output]\n",
        "    )\n",
        "\n",
        "    like_button.click(\n",
        "        lambda response: extract_place_names(response),\n",
        "        inputs=echo_output,\n",
        "        outputs=places_liked\n",
        "    )\n",
        "\n",
        "    dislike_button.click(\n",
        "        clear_outputs,\n",
        "        outputs=[sentence_output, echo_output]\n",
        "    )\n",
        "    map_button.click(\n",
        "        generate_route_map,\n",
        "        inputs=[places_liked],\n",
        "        outputs=[map_output]\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7867\n",
            "Running on public URL: https://cc248e378eeada7b85.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://cc248e378eeada7b85.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 793, in urlopen\n",
            "    response = self._make_request(\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 537, in _make_request\n",
            "    response = conn.getresponse()\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/urllib3/connection.py\", line 466, in getresponse\n",
            "    httplib_response = super().getresponse()\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
            "    response.begin()\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/lib/python3.10/http/client.py\", line 318, in begin\n",
            "    version, status, reason = self._read_status()\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/lib/python3.10/http/client.py\", line 287, in _read_status\n",
            "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
            "http.client.RemoteDisconnected: Remote end closed connection without response\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/requests/adapters.py\", line 486, in send\n",
            "    resp = conn.urlopen(\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 847, in urlopen\n",
            "    retries = retries.increment(\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/urllib3/util/retry.py\", line 470, in increment\n",
            "    raise reraise(type(error), error, _stacktrace)\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/urllib3/util/util.py\", line 38, in reraise\n",
            "    raise value.with_traceback(tb)\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 793, in urlopen\n",
            "    response = self._make_request(\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/urllib3/connectionpool.py\", line 537, in _make_request\n",
            "    response = conn.getresponse()\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/urllib3/connection.py\", line 466, in getresponse\n",
            "    httplib_response = super().getresponse()\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/lib/python3.10/http/client.py\", line 1375, in getresponse\n",
            "    response.begin()\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/lib/python3.10/http/client.py\", line 318, in begin\n",
            "    version, status, reason = self._read_status()\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/lib/python3.10/http/client.py\", line 287, in _read_status\n",
            "    raise RemoteDisconnected(\"Remote end closed connection without\"\n",
            "urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/gradio/queueing.py\", line 528, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/gradio/route_utils.py\", line 270, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/gradio/blocks.py\", line 1908, in process_api\n",
            "    result = await self.call_function(\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/gradio/blocks.py\", line 1485, in call_function\n",
            "    prediction = await anyio.to_thread.run_sync(\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2144, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 851, in run\n",
            "    result = context.run(func, *args)\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/gradio/utils.py\", line 808, in wrapper\n",
            "    response = f(*args, **kwargs)\n",
            "  File \"/var/folders/w8/w6hbyxsn14vf1h3mzjpcvr0r0000gn/T/ipykernel_52590/705277975.py\", line 94, in build_sentence_and_echo\n",
            "    processed_sentence = slow_echo(sentence)\n",
            "  File \"/var/folders/w8/w6hbyxsn14vf1h3mzjpcvr0r0000gn/T/ipykernel_52590/705277975.py\", line 86, in slow_echo\n",
            "    response = llm(message)\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py\", line 148, in warning_emitting_wrapper\n",
            "    return wrapped(*args, **kwargs)\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 1086, in __call__\n",
            "    self.generate(\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 803, in generate\n",
            "    output = self._generate_helper(\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 670, in _generate_helper\n",
            "    raise e\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 657, in _generate_helper\n",
            "    self._generate(\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/langchain_core/language_models/llms.py\", line 1317, in _generate\n",
            "    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/langchain_community/llms/huggingface_endpoint.py\", line 262, in _call\n",
            "    response = self.client.post(\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/huggingface_hub/inference/_client.py\", line 259, in post\n",
            "    response = get_session().post(\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/requests/sessions.py\", line 637, in post\n",
            "    return self.request(\"POST\", url, data=data, json=json, **kwargs)\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/requests/sessions.py\", line 589, in request\n",
            "    resp = self.send(prep, **send_kwargs)\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/requests/sessions.py\", line 703, in send\n",
            "    r = adapter.send(request, **kwargs)\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/huggingface_hub/utils/_http.py\", line 66, in send\n",
            "    return super().send(request, *args, **kwargs)\n",
            "  File \"/Users/user/.pyenv/versions/3.10.11/envs/dsenv/lib/python3.10/site-packages/requests/adapters.py\", line 501, in send\n",
            "    raise ConnectionError(err, request=request)\n",
            "requests.exceptions.ConnectionError: (ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response')), '(Request ID: 8addce05-6f66-400b-be13-84a98de79b1f)')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7867 <> https://cc248e378eeada7b85.gradio.live\n"
          ]
        }
      ],
      "source": [
        "#地圖的code\n",
        "import re\n",
        "import time\n",
        "import gradio as gr\n",
        "import requests\n",
        "\n",
        "import os\n",
        "import googlemaps\n",
        "from datetime import datetime\n",
        "import folium\n",
        "from folium import IFrame\n",
        "import base64\n",
        "from io import BytesIO\n",
        "import requests\n",
        "# import aspose.words as aw\n",
        "# from pathlib import Path\n",
        "\n",
        "# Initialize global variable to store liked places\n",
        "liked_places_set = set()\n",
        "\n",
        "# Google Maps API Key\n",
        "gmaps = googlemaps.Client(key=os.environ[\"GOOGLE_MAPS_API_KEY\"])\n",
        "\n",
        "def generate_route_map(places):\n",
        "\n",
        "    # Check if places is a string and convert to list\n",
        "    if isinstance(places, str):\n",
        "      places = [place.strip() for place in places.split(\",\")]\n",
        "\n",
        "    # Get coordinates for each place\n",
        "    coordinates = []\n",
        "    for place in places:\n",
        "      print(place)\n",
        "      geocode_result = gmaps.geocode(place)\n",
        "      if geocode_result:\n",
        "          coordinates.append(geocode_result[0]['geometry']['location'])\n",
        "      else:\n",
        "          print(f\"Geocode not found for: {place}\")\n",
        "\n",
        "    if len(coordinates) < 2:\n",
        "        return \"Not enough places to create a route.\"\n",
        "\n",
        "    # Generate the best route\n",
        "    waypoints = coordinates[1:-1]\n",
        "    directions_result = gmaps.directions(coordinates[0], coordinates[-1], waypoints=waypoints, mode=\"driving\", departure_time=datetime.now())\n",
        "\n",
        "    if not directions_result:\n",
        "        return \"No route found for the given places.\"\n",
        "\n",
        "    # Create a map\n",
        "    route_map = folium.Map(location=[coordinates[0]['lat'], coordinates[0]['lng']], zoom_start=12)\n",
        "\n",
        "    # Add markers to the map\n",
        "    for i, coordinate in enumerate(coordinates):\n",
        "        folium.Marker([coordinate['lat'], coordinate['lng']], tooltip=places[i]).add_to(route_map)\n",
        "\n",
        "    # Add route to the map\n",
        "    route = directions_result[0]['legs']\n",
        "    for leg in route:\n",
        "        for step in leg['steps']:\n",
        "            start_loc = step['start_location']\n",
        "            end_loc = step['end_location']\n",
        "            folium.PolyLine(locations=[(start_loc['lat'], start_loc['lng']), (end_loc['lat'], end_loc['lng'])], color='blue', weight=5).add_to(route_map)\n",
        "\n",
        "    # Save map as HTML\n",
        "    map_path = '/content/route_map.html'  # Save in Colab's file system\n",
        "    route_map.save(map_path)\n",
        "\n",
        "    return map_path\n",
        "\n",
        "# def html_to_jpg():\n",
        "#   doc = aw.Document(\"/content/route_map.html\")\n",
        "#   image_path = pathlib.Path('/content/route_image.jpg')\n",
        "#   doc.save(str(image_path))\n",
        "\n",
        "#   return image_path\n",
        "\n",
        "    # # Read the HTML content\n",
        "    # with open(map_path, 'r') as f:\n",
        "    #   html_content = f.read()\n",
        "\n",
        "    # return html_content\n",
        "\n",
        "def slow_echo(message, history=None):\n",
        "    time.sleep(0.05)\n",
        "    response = llm(message)\n",
        "    return response\n",
        "\n",
        "def sentence_builder(place, num, theme):\n",
        "    return f\"I would like to visit {' and '.join(place)}, and check out {num} cool places about {theme}. Please list {num} places.\"\n",
        "\n",
        "def build_sentence_and_echo(place, num, theme):\n",
        "    sentence = sentence_builder(place, num, theme)\n",
        "    processed_sentence = slow_echo(sentence)\n",
        "    return sentence, processed_sentence\n",
        "\n",
        "def extract_place_names(response):\n",
        "    # Regex to match the place names in the format '1. Name, Location:'\n",
        "    matches = re.findall(r\"\\d+\\.\\s*(.*?):\", response)\n",
        "    # # Join the extracted names with a comma and return\n",
        "    # place_names = ', '.join(matches)\n",
        "    # return place_names\n",
        "    return matches\n",
        "\n",
        "def update_liked_places(response):\n",
        "    global liked_places_set\n",
        "    new_places = extract_place_names(response)\n",
        "    liked_places_set.update(new_places)\n",
        "\n",
        "    return ', '.join(sorted(liked_places_set))\n",
        "\n",
        "def clear_outputs():\n",
        "    return \"\", \"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            place_input = gr.CheckboxGroup([\"基隆\", \"台北\", \"桃園\", \"新竹\", \"苗栗\", \"台中\", \"彰化\", \"南投\", \"雲林\", \"嘉義\", \"台南\", \"高雄\", \"屏東\", \"宜蘭\", \"花蓮\", \"台東\"], label=\"Places\", info=\"你想去哪？\")\n",
        "            num_input = gr.Slider(2, 5, value=3, step=1, label=\"去幾個地方？\", info=\"選2-5個\")\n",
        "            theme_input = gr.Dropdown(\n",
        "                [\"農場\", \"步道\", \"好吃的\", \"博物館\", \"宗教景點\", \"觀光工廠\", \"老街\"],\n",
        "                label=\"Theme\",\n",
        "                info=\"想去怎樣的地方呢？\"\n",
        "            )\n",
        "            build_button = gr.Button(\"快跟我講\")\n",
        "            sentence_output = gr.Textbox(label=\"我的需求\")\n",
        "            echo_output = gr.Textbox(label=\"LLM 說\")\n",
        "            like_button = gr.Button(\"就去這裡!\")\n",
        "            dislike_button = gr.Button(\"砍掉重練\")\n",
        "            #places_picked = gr.CheckboxGroup([\"Taipei\", \"Taichung\", \"Kaohsiung\"], label=\"Places\", info=\"Where do you wanna go?\")\n",
        "            places_liked = gr.Textbox(label=\"喜歡的地方\")\n",
        "            map_button = gr.Button(\"生成地圖\")\n",
        "            # show_button = gr.Button(\"Show Route Map\")\n",
        "            map_output = gr.File(label=\"點我看地圖\")\n",
        "            # map_image_output = gr.Image(label=\"Optimized Route Map Image\")\n",
        "        with gr.Column():\n",
        "            chatbot = gr.ChatInterface(slow_echo).queue()\n",
        "\n",
        "    build_button.click(\n",
        "        build_sentence_and_echo,\n",
        "        inputs=[place_input, num_input, theme_input],\n",
        "        outputs=[sentence_output, echo_output]\n",
        "    )\n",
        "\n",
        "    like_button.click(\n",
        "        update_liked_places,\n",
        "        inputs=echo_output,\n",
        "        outputs=places_liked\n",
        "    )\n",
        "\n",
        "    dislike_button.click(\n",
        "        clear_outputs,\n",
        "        outputs=[sentence_output, echo_output]\n",
        "    )\n",
        "    map_button.click(\n",
        "        generate_route_map,\n",
        "        inputs=[places_liked],\n",
        "        outputs=[map_output]\n",
        "    )\n",
        "\n",
        "    # show_button.click(\n",
        "    #     html_to_jpg,\n",
        "    #     outputs=[map_image_output]\n",
        "    # )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug = True, share = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- 爬完全部頁面\n",
        "- Gradio\n",
        "    - 選單\n",
        "- 地圖\n",
        "    - Google Maps API (200USD/month額度)\n",
        "### 5/30 Thu 盡量做完自己的部分+確認有沒有問題\n",
        "### 6/2 Sun 做完project\n",
        "### 6/4 Tue 做完簡報\n",
        "### 6/5W 簡報上傳\n",
        "### 6/6T 報告\n",
        "```\n",
        "Gradio\n",
        "地點 [台中][台南][高雄]\n",
        "類型 [戶外][博物館][海邊] \n",
        "----------- {導出選項}\n",
        "--> （給langchain）我要去___，___的景點，可以請你推薦我一些地方嗎？...\n",
        "--> (langchain) 推薦 x y z 景點 {(1)限制prompt的輸出 如果不行的話，試(2)sensetagger}\n",
        "--> (user) [x][],[y][V],[z][V] {做出可勾選的回覆}\n",
        "-----------\n",
        "--> (langchain) 目前有[y][z]（存在其他地方），請問還有其他想去的地方嗎？\n",
        "（重複前面步驟+記憶已選地點在對話輸入匡下面{建立output欄}）\n",
        "（使用者按output欄旁邊的「我選好了～」，地點選擇過程中止{}）\n",
        "（開始路徑規劃選擇）\n",
        "--> (langchain) 請問您偏好的旅程...? \n",
        "--> (user選)[1][2][3]\n",
        "--> (output) 正在用[1]的方式為您生成最佳路徑\n",
        "------以上是Tiff的部分\n",
        "\n",
        "{導入google maps到gradio}{導出output的地點}{用google maps規劃路徑}{輸出地圖}\n",
        "--> (langchain) 好的，以下是我規劃的路徑：\n",
        "[地圖出現]（[->導出成mymaps]）\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
